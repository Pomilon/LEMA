{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEMA Benchmark Notebook\n",
        "This notebook was auto-generated to verify the LEMA framework on Kaggle GPUs.\n",
        "It compares Standard Fine-Tuning (PEFT) vs LEMA (Virtual Memory) VRAM usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!mkdir -p src/lema/core src/lema/engine src/lema/models src/lema/utils tasks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/config.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional, Dict, Any\n",
        "from enum import Enum\n",
        "\n",
        "class MemoryStrategy(Enum):\n",
        "    STREAMING = \"streaming\" # Disk -> RAM -> VRAM\n",
        "    RESIDENT = \"resident\"   # RAM -> VRAM (No Disk offload for weights)\n",
        "\n",
        "@dataclass\n",
        "class LemaConfig:\n",
        "    \"\"\"\n",
        "    Central Configuration for LEMA Training/Inference.\n",
        "    \"\"\"\n",
        "    # Model Settings\n",
        "    model_name_or_path: str\n",
        "    gbi_path: Optional[str] = None # Path to converted safetensors for GBI\n",
        "    \n",
        "    # Hardware / Memory Settings\n",
        "    device: str = \"cuda\"\n",
        "    strategy: MemoryStrategy = MemoryStrategy.STREAMING\n",
        "    ram_buffer_size: int = 2 # Number of layers to keep in RAM\n",
        "    vram_buffer_size: int = 1 # Number of layers to keep in VRAM\n",
        "    \n",
        "    # LoRA Settings\n",
        "    use_lora: bool = True\n",
        "    lora_rank: int = 16\n",
        "    lora_alpha: int = 32\n",
        "    lora_target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
        "    \n",
        "    # Training Settings\n",
        "    learning_rate: float = 1e-4\n",
        "    batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_seq_length: int = 512\n",
        "    gradient_checkpointing: bool = False\n",
        "    \n",
        "    # Advanced\n",
        "    dtype: str = \"float16\" # float16, bfloat16, float32\n",
        "    attn_implementation: str = \"eager\" # eager, sdpa, flash_attention_2\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.gbi_path is None:\n",
        "            # Default to expecting a local safetensors file named after the model or a standard name\n",
        "            if self.model_name_or_path.endswith(\".safetensors\"):\n",
        "                self.gbi_path = self.model_name_or_path\n",
        "            else:\n",
        "                self.gbi_path = \"model.safetensors\"\n",
        "        \n",
        "        if isinstance(self.strategy, str):\n",
        "            self.strategy = MemoryStrategy(self.strategy.lower())\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            k: v.value if isinstance(v, Enum) else v \n",
        "            for k, v in self.__dict__.items()\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/engine/trainer.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import threading\n",
        "from typing import Any, Optional, List, Union\n",
        "from ..core.memory import TripleBufferManager\n",
        "from ..models.base import LemaModelAdapter\n",
        "from ..config import LemaConfig, MemoryStrategy\n",
        "\n",
        "class LemaTrainer:\n",
        "    def __init__(self, \n",
        "                 config: LemaConfig,\n",
        "                 model_adapter: LemaModelAdapter, \n",
        "                 gbi, \n",
        "                 lora_manager=None, \n",
        "                 optimizer=None):\n",
        "        \n",
        "        self.config = config\n",
        "        self.adapter = model_adapter\n",
        "        self.gbi = gbi\n",
        "        self.device = config.device\n",
        "        self.strategy = config.strategy\n",
        "        \n",
        "        # Initialize Memory Manager with config strategy\n",
        "        # Note: TripleBufferManager might need updates to accept config too, \n",
        "        # but for now we map config values to its expected args\n",
        "        self.memory = TripleBufferManager(gbi, model_adapter, self.device, strategy=self.strategy)\n",
        "        \n",
        "        self.layers = self.adapter.get_layer_metadata()\n",
        "        self.lora_manager = lora_manager\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def train_step(self, inputs: Any, labels: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Executes one forward pass and one backward pass.\n",
        "        If labels are provided, computes CrossEntropyLoss.\n",
        "        \"\"\"\n",
        "        boundary_activations: List[torch.Tensor] = []\n",
        "        is_streaming = (self.strategy == MemoryStrategy.STREAMING)\n",
        "        \n",
        "        # --- FORWARD PASS ---\n",
        "        if is_streaming:\n",
        "            self.memory.prefetch_to_ram(self.layers[0]['id'], 0)\n",
        "            self.memory.async_transfer_to_vram(self.layers[0]['id'], 0, ram_slot=0)\n",
        "            if len(self.layers) > 1:\n",
        "                self.memory.prefetch_to_ram(self.layers[1]['id'], 1)\n",
        "        else:\n",
        "            self.memory.async_transfer_to_vram(self.layers[0]['id'], 0)\n",
        "\n",
        "        hidden_states = inputs\n",
        "        \n",
        "        for i, layer_meta in enumerate(self.layers):\n",
        "            slot = i % 2\n",
        "            next_slot = (i + 1) % 2\n",
        "            \n",
        "            flat_vram = self.memory.get_vram_flat_buffer(slot)\n",
        "            \n",
        "            disk_thread = None\n",
        "            if i + 1 < len(self.layers):\n",
        "                if is_streaming:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i+1]['id'], next_slot, ram_slot=next_slot)\n",
        "                    if i + 2 < len(self.layers):\n",
        "                        disk_thread = threading.Thread(target=self.memory.prefetch_to_ram, args=(self.layers[i+2]['id'], slot))\n",
        "                        disk_thread.start()\n",
        "                else:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i+1]['id'], next_slot)\n",
        "            \n",
        "            layer_module = self.adapter.construct_layer_module(layer_meta['id'], flat_vram, self.lora_manager)\n",
        "            \n",
        "            # Store input for backward\n",
        "            if isinstance(hidden_states, tuple): \n",
        "                 current_input = hidden_states[0].detach()\n",
        "            else:\n",
        "                current_input = hidden_states.detach()\n",
        "            boundary_activations.append(current_input)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                hidden_states = self.adapter.forward_layer(layer_module, hidden_states, gradient_checkpointing=False)\n",
        "\n",
        "            if disk_thread: disk_thread.join()\n",
        "            if hasattr(self.adapter, \"release_layer_module\"):\n",
        "                self.adapter.release_layer_module(layer_module)\n",
        "            del layer_module\n",
        "\n",
        "        # Final Logits\n",
        "        logits = hidden_states\n",
        "        loss_val = None\n",
        "\n",
        "        # --- BACKWARD PASS ---\n",
        "        if not torch.is_grad_enabled():\n",
        "            return logits, None\n",
        "\n",
        "        last_idx = len(self.layers) - 1\n",
        "        if is_streaming:\n",
        "            self.memory.prefetch_to_ram(self.layers[last_idx]['id'], 0)\n",
        "            self.memory.async_transfer_to_vram(self.layers[last_idx]['id'], 0, ram_slot=0)\n",
        "            if last_idx > 0:\n",
        "                self.memory.prefetch_to_ram(self.layers[last_idx-1]['id'], 1)\n",
        "        else:\n",
        "            self.memory.async_transfer_to_vram(self.layers[last_idx]['id'], 0)\n",
        "        \n",
        "        grad_output = None\n",
        "        \n",
        "        for i in range(last_idx, -1, -1):\n",
        "            slot = (last_idx - i) % 2\n",
        "            next_slot = (last_idx - i + 1) % 2\n",
        "            \n",
        "            flat_vram = self.memory.get_vram_flat_buffer(slot)\n",
        "            \n",
        "            disk_thread = None\n",
        "            if i - 1 >= 0:\n",
        "                if is_streaming:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i-1]['id'], next_slot, ram_slot=next_slot)\n",
        "                    if i - 2 >= 0:\n",
        "                        disk_thread = threading.Thread(target=self.memory.prefetch_to_ram, args=(self.layers[i-2]['id'], slot))\n",
        "                        disk_thread.start()\n",
        "                else:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i-1]['id'], next_slot)\n",
        "            \n",
        "            layer_module = self.adapter.construct_layer_module(self.layers[i]['id'], flat_vram, self.lora_manager)\n",
        "            layer_input = boundary_activations[i]\n",
        "            if layer_input.dtype.is_floating_point:\n",
        "                layer_input.requires_grad_(True)\n",
        "            \n",
        "            output = self.adapter.forward_layer(layer_module, layer_input, gradient_checkpointing=self.config.gradient_checkpointing)\n",
        "            \n",
        "            if i == last_idx:\n",
        "                if labels is not None:\n",
        "                    # Real Causal LM Loss\n",
        "                    # Shift so that tokens < n predict n\n",
        "                    shift_logits = output[..., :-1, :].contiguous()\n",
        "                    shift_labels = labels[..., 1:].contiguous()\n",
        "                    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "                    loss_val = loss.item()\n",
        "                else:\n",
        "                    loss = output.mean() # Dummy\n",
        "                \n",
        "                loss.backward()\n",
        "                grad_output = layer_input.grad\n",
        "            else:\n",
        "                if isinstance(output, tuple):\n",
        "                    output[0].backward(grad_output)\n",
        "                else:\n",
        "                    output.backward(grad_output)\n",
        "                grad_output = layer_input.grad\n",
        "            \n",
        "            if disk_thread: disk_thread.join()\n",
        "            if hasattr(self.adapter, \"release_layer_module\"):\n",
        "                self.adapter.release_layer_module(layer_module)\n",
        "            del layer_module\n",
        "\n",
        "        if self.optimizer:\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "        return logits, loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/lora.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "try:\n",
        "    from transformers.pytorch_utils import Conv1D\n",
        "except ImportError:\n",
        "    Conv1D = None\n",
        "\n",
        "class LoRAWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps a Linear or Conv1D layer with LoRA adapters.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_layer: nn.Module, rank: int, alpha: float, lora_A: nn.Parameter, lora_B: nn.Parameter):\n",
        "        super().__init__()\n",
        "        self.base_layer = base_layer\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank\n",
        "        \n",
        "        self.lora_A = lora_A\n",
        "        self.lora_B = lora_B\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Base forward\n",
        "        result = self.base_layer(x)\n",
        "        \n",
        "        # LoRA forward\n",
        "        # Calculation: (x @ A.T @ B.T) * scaling\n",
        "        lora_out = (x @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
        "        return result + lora_out\n",
        "\n",
        "class LoRAManager:\n",
        "    \"\"\"\n",
        "    Manages the lifecycle and storage of LoRA parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict, device=\"cuda\"):\n",
        "        self.rank = config.get(\"r\", 8)\n",
        "        self.alpha = config.get(\"alpha\", 16)\n",
        "        self.target_modules = config.get(\"target_modules\", [\"c_attn\", \"c_proj\", \"c_fc\"])\n",
        "        self.device = device\n",
        "        \n",
        "        # Store parameters: key -> {'A': Param, 'B': Param}\n",
        "        self.params: Dict[str, Dict[str, nn.Parameter]] = {}\n",
        "        \n",
        "    def get_or_init_params(self, layer_id: int, module_name: str, in_features: int, out_features: int) -> Dict[str, nn.Parameter]:\n",
        "        key = f\"{layer_id}.{module_name}\"\n",
        "        \n",
        "        if key not in self.params:\n",
        "            lora_A = torch.zeros((self.rank, in_features), device=self.device)\n",
        "            nn.init.kaiming_uniform_(lora_A, a=math.sqrt(5))\n",
        "            \n",
        "            lora_B = torch.zeros((out_features, self.rank), device=self.device)\n",
        "            nn.init.zeros_(lora_B)\n",
        "            \n",
        "            self.params[key] = {\n",
        "                'A': nn.Parameter(lora_A, requires_grad=True),\n",
        "                'B': nn.Parameter(lora_B, requires_grad=True)\n",
        "            }\n",
        "            \n",
        "        return self.params[key]\n",
        "\n",
        "    def apply_lora(self, layer_id: int, module: nn.Module, module_name_prefix: str = \"\"):\n",
        "        \"\"\"\n",
        "        Recursively replaces Linear/Conv1D layers with LoRAWrapper if they match target_modules.\n",
        "        \"\"\"\n",
        "        for name, child in module.named_children():\n",
        "            full_name = f\"{module_name_prefix}.{name}\" if module_name_prefix else name\n",
        "            \n",
        "            # Check if this is a target module\n",
        "            is_target = any(name == target or name.endswith(target) for target in self.target_modules)\n",
        "            \n",
        "            if isinstance(child, LoRAWrapper) and is_target:\n",
        "                # Already wrapped, just swap parameters for the new layer\n",
        "                if isinstance(child.base_layer, nn.Linear):\n",
        "                    in_features = child.base_layer.in_features\n",
        "                    out_features = child.base_layer.out_features\n",
        "                elif Conv1D is not None and isinstance(child.base_layer, Conv1D):\n",
        "                    in_features = child.base_layer.weight.shape[0]\n",
        "                    out_features = child.base_layer.weight.shape[1]\n",
        "                else:\n",
        "                    # Generic fallback if weight exists\n",
        "                    in_features = child.base_layer.weight.shape[1] if hasattr(child.base_layer, \"weight\") else 0\n",
        "                    out_features = child.base_layer.weight.shape[0] if hasattr(child.base_layer, \"weight\") else 0\n",
        "\n",
        "                params = self.get_or_init_params(layer_id, full_name, in_features, out_features)\n",
        "                child.lora_A = params['A']\n",
        "                child.lora_B = params['B']\n",
        "                continue\n",
        "\n",
        "            in_features = None\n",
        "            out_features = None\n",
        "            \n",
        "            if isinstance(child, nn.Linear) and is_target:\n",
        "                in_features = child.in_features\n",
        "                out_features = child.out_features\n",
        "            elif Conv1D is not None and isinstance(child, Conv1D) and is_target:\n",
        "                in_features = child.weight.shape[0]\n",
        "                out_features = child.weight.shape[1]\n",
        "                \n",
        "            if in_features is not None and out_features is not None:\n",
        "                params = self.get_or_init_params(layer_id, full_name, in_features, out_features)\n",
        "                \n",
        "                lora_layer = LoRAWrapper(\n",
        "                    base_layer=child,\n",
        "                    rank=self.rank,\n",
        "                    alpha=self.alpha,\n",
        "                    lora_A=params['A'],\n",
        "                    lora_B=params['B']\n",
        "                )\n",
        "                setattr(module, name, lora_layer)\n",
        "            else:\n",
        "                self.apply_lora(layer_id, child, full_name)\n",
        "\n",
        "    def update_lora_params(self, layer_id: int, module: nn.Module):\n",
        "        \"\"\"\n",
        "        Efficiently updates LoRA parameters for a reused module.\n",
        "        Uses cached wrapper list if available, otherwise traverses and builds cache.\n",
        "        \"\"\"\n",
        "        if not hasattr(module, \"_lora_cache\"):\n",
        "            module._lora_cache = []\n",
        "            # First time: Traverse and collect wrappers\n",
        "            # We reuse apply_lora logic but adapted for collection\n",
        "            self._collect_and_update_wrappers(layer_id, module, \"\", module._lora_cache)\n",
        "        else:\n",
        "            # Fast path: Update parameters from cache\n",
        "            for wrapper, name, in_f, out_f in module._lora_cache:\n",
        "                params = self.get_or_init_params(layer_id, name, in_f, out_f)\n",
        "                wrapper.lora_A = params['A']\n",
        "                wrapper.lora_B = params['B']\n",
        "\n",
        "    def _collect_and_update_wrappers(self, layer_id: int, module: nn.Module, prefix: str, cache: List):\n",
        "        for name, child in module.named_children():\n",
        "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "            \n",
        "            if isinstance(child, LoRAWrapper):\n",
        "                # Already wrapped (from previous usage or just now)\n",
        "                in_f = child.base_layer.in_features if hasattr(child.base_layer, \"in_features\") else child.base_layer.weight.shape[1]\n",
        "                out_f = child.base_layer.out_features if hasattr(child.base_layer, \"out_features\") else child.base_layer.weight.shape[0]\n",
        "                \n",
        "                params = self.get_or_init_params(layer_id, full_name, in_f, out_f)\n",
        "                child.lora_A = params['A']\n",
        "                child.lora_B = params['B']\n",
        "                \n",
        "                cache.append((child, full_name, in_f, out_f))\n",
        "                continue\n",
        "            \n",
        "            # Check if this is a target module to wrap\n",
        "            is_target = any(name == target or name.endswith(target) for target in self.target_modules)\n",
        "            \n",
        "            if is_target and (isinstance(child, nn.Linear) or (Conv1D is not None and isinstance(child, Conv1D))):\n",
        "                in_features = child.in_features if isinstance(child, nn.Linear) else child.weight.shape[0]\n",
        "                out_features = child.out_features if isinstance(child, nn.Linear) else child.weight.shape[1]\n",
        "                \n",
        "                params = self.get_or_init_params(layer_id, full_name, in_features, out_features)\n",
        "                \n",
        "                lora_layer = LoRAWrapper(\n",
        "                    base_layer=child,\n",
        "                    rank=self.rank,\n",
        "                    alpha=self.alpha,\n",
        "                    lora_A=params['A'],\n",
        "                    lora_B=params['B']\n",
        "                )\n",
        "                setattr(module, name, lora_layer)\n",
        "                cache.append((lora_layer, full_name, in_features, out_features))\n",
        "            else:\n",
        "                self._collect_and_update_wrappers(layer_id, child, full_name, cache)\n",
        "\n",
        "    def get_trainable_parameters(self) -> List[torch.nn.Parameter]:\n",
        "        \"\"\"\n",
        "        Returns a list of all nn.Parameter objects managed by this manager.\n",
        "        \"\"\"\n",
        "        all_params = []\n",
        "        for p_dict in self.params.values():\n",
        "            all_params.append(p_dict['A'])\n",
        "            all_params.append(p_dict['B'])\n",
        "        return all_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/gbi.py\n",
        "import torch\n",
        "from safetensors import safe_open\n",
        "from typing import Dict, List, Any, Optional\n",
        "import os\n",
        "\n",
        "class GlobalBinaryIndex:\n",
        "    \"\"\"\n",
        "    GBI v0.4: Contiguous Block Access.\n",
        "    Allows fetching a whole layer as a single byte-range if possible,\n",
        "    but here we focus on providing tensors for contiguous packing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "        \n",
        "        self.model_path = model_path\n",
        "        self.handle = safe_open(self.model_path, framework=\"pt\", device=\"cpu\")\n",
        "        self.keys = list(self.handle.keys())\n",
        "\n",
        "    def load_tensors(self, param_names: List[str], device: str = \"cpu\") -> Dict[str, torch.Tensor]:\n",
        "        tensors = {}\n",
        "        for name in param_names:\n",
        "            tensors[name] = self.handle.get_tensor(name)\n",
        "        return tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/memory.py\n",
        "import torch\n",
        "import threading\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "from enum import Enum\n",
        "import gc\n",
        "from ..config import MemoryStrategy\n",
        "\n",
        "class TripleBufferManager:\n",
        "    \"\"\"\n",
        "    Unified Memory Manager supporting both Disk-Streaming and RAM-Residency.\n",
        "    \"\"\"\n",
        "    def __init__(self, gbi, adapter, device=\"cuda\", strategy=MemoryStrategy.STREAMING):\n",
        "        self.gbi = gbi\n",
        "        self.adapter = adapter\n",
        "        self.device = device\n",
        "        self.strategy = strategy\n",
        "        \n",
        "        self.is_cuda = self.device.startswith(\"cuda\")\n",
        "        self.transfer_streams = [torch.cuda.Stream() for _ in range(2)] if self.is_cuda else None\n",
        "        \n",
        "        self.layers_meta = self.adapter.get_layer_metadata()\n",
        "        \n",
        "        # Calculate max layer size for pre-allocating buffers\n",
        "        self.max_params = self._calculate_max_params()\n",
        "        \n",
        "        # Pre-allocated VRAM slots (Double buffering)\n",
        "        self.vram_flat_buffers = [\n",
        "            torch.empty(self.max_params, device=self.device, dtype=torch.float32)\n",
        "            for _ in range(2)\n",
        "        ]\n",
        "        \n",
        "        # RAM Buffers\n",
        "        if self.strategy == MemoryStrategy.RESIDENT:\n",
        "            print(f\"LEMA: Initializing RESIDENT strategy (Caching model in RAM)...\")\n",
        "            self.ram_flat_buffers: Dict[int, torch.Tensor] = {}\n",
        "            self._initialize_full_ram_cache()\n",
        "        else:\n",
        "            print(f\"LEMA: Initializing STREAMING strategy (Default)...\")\n",
        "            # In streaming mode, we only need 2 RAM slots for the pipeline\n",
        "            self.ram_flat_buffers: List[torch.Tensor] = [\n",
        "                torch.empty(self.max_params, device=\"cpu\", dtype=torch.float32).pin_memory() if self.is_cuda else torch.empty(self.max_params, device=\"cpu\", dtype=torch.float32)\n",
        "                for _ in range(2)\n",
        "            ]\n",
        "            self.ram_layer_ids = [-1, -1]\n",
        "\n",
        "    def _calculate_max_params(self) -> int:\n",
        "        max_p = 0\n",
        "        for layer in self.layers_meta:\n",
        "            names = self.adapter.get_param_names_for_layer(layer['id'])\n",
        "            current_p = 0\n",
        "            for name in names:\n",
        "                meta = self.gbi.handle.get_slice(name)\n",
        "                current_p += meta.get_shape().numel() if hasattr(meta.get_shape(), 'numel') else torch.Size(meta.get_shape()).numel()\n",
        "            max_p = max(max_p, current_p)\n",
        "        return max_p\n",
        "\n",
        "    def _initialize_full_ram_cache(self):\n",
        "        \"\"\"Pre-packs the entire model into pinned RAM.\"\"\"\n",
        "        for layer in self.layers_meta:\n",
        "            layer_id = layer['id']\n",
        "            self._pack_layer_to_ram(layer_id, is_resident=True)\n",
        "\n",
        "    def _pack_layer_to_ram(self, layer_id: int, slot: int = 0, is_resident: bool = False):\n",
        "        \"\"\"Helper to load a layer from disk and pack it into a flat RAM buffer.\"\"\"\n",
        "        param_names = self.adapter.get_param_names_for_layer(layer_id)\n",
        "        weights = self.gbi.load_tensors(param_names, device=\"cpu\")\n",
        "        \n",
        "        if is_resident:\n",
        "            total_el = sum(w.numel() for w in weights.values())\n",
        "            buf = torch.empty(total_el, device=\"cpu\", dtype=torch.float32).pin_memory()\n",
        "        else:\n",
        "            buf = self.ram_flat_buffers[slot]\n",
        "            \n",
        "        offset = 0\n",
        "        for name in param_names:\n",
        "            w = weights[name]\n",
        "            numel = w.numel()\n",
        "            buf[offset : offset + numel].copy_(w.view(-1))\n",
        "            offset += numel\n",
        "            \n",
        "        if is_resident:\n",
        "            self.ram_flat_buffers[layer_id] = buf\n",
        "        else:\n",
        "            self.ram_layer_ids[slot] = layer_id\n",
        "\n",
        "    def prefetch_to_ram(self, layer_id: int, ram_slot: int):\n",
        "        \"\"\"Stage 1 (Streaming only): Load from Disk to RAM Slot.\"\"\"\n",
        "        if self.strategy == MemoryStrategy.RESIDENT:\n",
        "            return # No-op for resident mode\n",
        "            \n",
        "        if self.ram_layer_ids[ram_slot] == layer_id:\n",
        "            return\n",
        "        \n",
        "        self._pack_layer_to_ram(layer_id, ram_slot, is_resident=False)\n",
        "\n",
        "    def async_transfer_to_vram(self, layer_id: int, vram_slot: int, ram_slot: Optional[int] = None):\n",
        "        \"\"\"Stage 2: Async transfer to GPU.\"\"\"\n",
        "        if self.strategy == MemoryStrategy.RESIDENT:\n",
        "            src_buf = self.ram_flat_buffers[layer_id]\n",
        "        else:\n",
        "            if ram_slot is None:\n",
        "                raise ValueError(\"ram_slot must be provided in streaming mode\")\n",
        "            src_buf = self.ram_flat_buffers[ram_slot]\n",
        "            \n",
        "        vram_dest = self.vram_flat_buffers[vram_slot]\n",
        "        \n",
        "        if self.is_cuda and self.transfer_streams:\n",
        "            stream = self.transfer_streams[vram_slot]\n",
        "            with torch.cuda.stream(stream):\n",
        "                vram_dest[:src_buf.numel()].copy_(src_buf, non_blocking=True)\n",
        "        else:\n",
        "            # CPU or Synchronous copy\n",
        "            vram_dest[:src_buf.numel()].copy_(src_buf)\n",
        "\n",
        "    def get_vram_flat_buffer(self, vram_slot: int) -> torch.Tensor:\n",
        "        \"\"\"Stage 3: Usage.\"\"\"\n",
        "        if self.is_cuda and self.transfer_streams:\n",
        "            self.transfer_streams[vram_slot].synchronize()\n",
        "        return self.vram_flat_buffers[vram_slot]\n",
        "\n",
        "    def clear_vram_slot(self, vram_slot: int):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/base.py\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LemaModelAdapter(ABC):\n",
        "    \"\"\"\n",
        "    Abstract Base Class for LEMA Model Adapters.\n",
        "    \n",
        "    This class defines the interface that any model architecture must implement\n",
        "    to be compatible with the LEMA (Layer-wise Efficient Memory Abstraction) framework.\n",
        "    It bridges the gap between the raw binary weights managed by LEMA and the \n",
        "    PyTorch execution semantics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_layer_metadata(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Returns a list of dictionaries, where each dictionary describes a logical \"layer\"\n",
        "        or \"block\" in the model that LEMA should manage as a unit.\n",
        "        \n",
        "        Returns:\n",
        "            List[Dict]: e.g. [{'id': 0, 'name': 'transformer.h.0', 'inputs': [...], 'outputs': [...]}, ...]\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def construct_layer_module(self, layer_id: int, weights: Dict[str, torch.Tensor], lora_manager: Optional[Any] = None) -> nn.Module:\n",
        "        \"\"\"\n",
        "        Constructs a PyTorch nn.Module for the specified layer using the provided weights.\n",
        "        The weights will be on the target device (VRAM) when passed here.\n",
        "        \n",
        "        Args:\n",
        "            layer_id (int): The index of the layer to construct.\n",
        "            weights (Dict[str, torch.Tensor]): A dictionary mapping parameter names to tensors.\n",
        "            lora_manager (Optional[Any]): The LoRAManager instance to apply adapters.\n",
        "            \n",
        "        Returns:\n",
        "            nn.Module: The executable layer module.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward_layer(self, layer_module: nn.Module, inputs: Any, **kwargs) -> Any:\n",
        "        \"\"\"\n",
        "        Executes the forward pass for a single layer.\n",
        "        \n",
        "        Args:\n",
        "            layer_module (nn.Module): The module constructed by construct_layer_module.\n",
        "            inputs (Any): The input activations (tensor or tuple of tensors).\n",
        "            **kwargs: Additional arguments (e.g., attention masks, rotary embeddings).\n",
        "            \n",
        "        Returns:\n",
        "            Any: The output activations.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_param_names_for_layer(self, layer_id: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Returns the list of parameter names (as found in the safetensors file) \n",
        "        required for the specified layer.\n",
        "        \n",
        "        Args:\n",
        "            layer_id (int): Layer index.\n",
        "            \n",
        "        Returns:\n",
        "            List[str]: List of parameter keys.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def hidden_size(self) -> int:\n",
        "        \"\"\"Returns the model's hidden size for buffer allocation.\"\"\"\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/gpt2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Config\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from .base import LemaModelAdapter\n",
        "\n",
        "class GPT2Adapter(LemaModelAdapter):\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__(config)\n",
        "        self.hf_config = GPT2Config(**config)\n",
        "        if getattr(self.hf_config, \"_attn_implementation\", None) is None:\n",
        "            self.hf_config._attn_implementation = config.get(\"attn_implementation\", \"eager\")\n",
        "        self.layer_pool: List[nn.Module] = []\n",
        "        self.param_mappings: Dict[int, List[tuple]] = {}\n",
        "        \n",
        "    def get_layer_metadata(self) -> List[Dict[str, Any]]:\n",
        "        layers = []\n",
        "        layers.append({'id': 0, 'name': 'embeddings', 'type': 'embedding'})\n",
        "        for i in range(self.hf_config.n_layer):\n",
        "            layers.append({'id': i + 1, 'name': f'h.{i}', 'type': 'block', 'block_index': i})\n",
        "        layers.append({'id': self.hf_config.n_layer + 1, 'name': 'head', 'type': 'head'})\n",
        "        return layers\n",
        "\n",
        "    def get_param_names_for_layer(self, layer_id: int) -> List[str]:\n",
        "        if layer_id == 0:\n",
        "            return ['transformer.wte.weight', 'transformer.wpe.weight']\n",
        "        elif 1 <= layer_id <= self.hf_config.n_layer:\n",
        "            idx = layer_id - 1\n",
        "            prefix = f\"transformer.h.{idx}\"\n",
        "            return [\n",
        "                f\"{prefix}.attn.c_attn.weight\", f\"{prefix}.attn.c_attn.bias\",\n",
        "                f\"{prefix}.attn.c_proj.weight\", f\"{prefix}.attn.c_proj.bias\",\n",
        "                f\"{prefix}.ln_1.weight\", f\"{prefix}.ln_1.bias\",\n",
        "                f\"{prefix}.ln_2.weight\", f\"{prefix}.ln_2.bias\",\n",
        "                f\"{prefix}.mlp.c_fc.weight\", f\"{prefix}.mlp.c_fc.bias\",\n",
        "                f\"{prefix}.mlp.c_proj.weight\", f\"{prefix}.mlp.c_proj.bias\",\n",
        "            ]\n",
        "        elif layer_id == self.hf_config.n_layer + 1:\n",
        "            return ['transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight']\n",
        "        return []\n",
        "\n",
        "    def construct_layer_module(self, layer_id: int, flat_buffer: Optional[torch.Tensor] = None, lora_manager: Optional[Any] = None) -> nn.Module:\n",
        "        device = flat_buffer.device if flat_buffer is not None else torch.device(\"cpu\")\n",
        "        module = None\n",
        "        for i, m in enumerate(self.layer_pool):\n",
        "            if layer_id == 0 and isinstance(m, GPT2EmbeddingsLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif layer_id == self.hf_config.n_layer + 1 and isinstance(m, GPT2HeadLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif 1 <= layer_id <= self.hf_config.n_layer and isinstance(m, GPT2Block):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "        \n",
        "        if module is None:\n",
        "            if layer_id == 0: module = GPT2EmbeddingsLayer(self.hf_config)\n",
        "            elif layer_id == self.hf_config.n_layer + 1: module = GPT2HeadLayer(self.hf_config)\n",
        "            else:\n",
        "                module = GPT2Block(self.hf_config)\n",
        "            \n",
        "            # Initialization only\n",
        "            module.to(device)\n",
        "\n",
        "        if lora_manager and 1 <= layer_id <= self.hf_config.n_layer:\n",
        "            lora_manager.update_lora_params(layer_id, module)\n",
        "\n",
        "        if id(module) not in self.param_mappings:\n",
        "            self.param_mappings[id(module)] = self._create_mapping(layer_id, module)\n",
        "\n",
        "        if flat_buffer is not None:\n",
        "            mapping = self.param_mappings[id(module)]\n",
        "            offset = 0\n",
        "            with torch.no_grad():\n",
        "                for param, numel, shape in mapping:\n",
        "                    param.data.copy_(flat_buffer[offset : offset + numel].view(shape), non_blocking=True)\n",
        "                    offset += numel\n",
        "            \n",
        "        return module\n",
        "\n",
        "    def _create_mapping(self, layer_id: int, module: nn.Module) -> List[tuple]:\n",
        "        names = self.get_param_names_for_layer(layer_id)\n",
        "        idx = layer_id - 1\n",
        "        module_params = dict(module.named_parameters())\n",
        "        mapping = []\n",
        "        for full_name in names:\n",
        "            if layer_id == 0:\n",
        "                clean_k = \"wte.weight\" if \"wte\" in full_name else \"wpe.weight\"\n",
        "            elif layer_id == self.hf_config.n_layer + 1:\n",
        "                if \"ln_f\" in full_name: clean_k = \"ln_f.weight\" if \"weight\" in full_name else \"ln_f.bias\"\n",
        "                else: clean_k = \"head.weight\"\n",
        "            else:\n",
        "                prefix = f\"transformer.h.{idx}.\"\n",
        "                clean_k = full_name[len(prefix):]\n",
        "                if clean_k not in module_params: clean_k = clean_k.replace(\".weight\", \".base_layer.weight\").replace(\".bias\", \".base_layer.bias\")\n",
        "            param = module_params[clean_k]\n",
        "            mapping.append((param, param.numel(), param.shape))\n",
        "        return mapping\n",
        "\n",
        "    def release_layer_module(self, module: nn.Module):\n",
        "        if len(self.layer_pool) < 5:\n",
        "            self.layer_pool.append(module)\n",
        "\n",
        "    def forward_layer(self, layer_module: nn.Module, inputs: Any, **kwargs) -> Any:\n",
        "        hidden_states = inputs[0] if isinstance(inputs, tuple) else inputs\n",
        "        if isinstance(layer_module, GPT2Block):\n",
        "            return layer_module(hidden_states)[0]\n",
        "        return layer_module(hidden_states)\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.hf_config.n_embd\n",
        "\n",
        "class GPT2EmbeddingsLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "    def forward(self, input_ids):\n",
        "        position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "        return self.wte(input_ids) + self.wpe(position_ids)\n",
        "\n",
        "class GPT2HeadLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "    def forward(self, x): return self.head(self.ln_f(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/llama.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm, LlamaConfig, LlamaRotaryEmbedding\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from .base import LemaModelAdapter\n",
        "\n",
        "class LlamaAdapter(LemaModelAdapter):\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__(config)\n",
        "        self.hf_config = LlamaConfig(**config)\n",
        "        if getattr(self.hf_config, \"_attn_implementation\", None) is None:\n",
        "            self.hf_config._attn_implementation = config.get(\"attn_implementation\", \"eager\")\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(self.hf_config)\n",
        "        self.layer_pool: List[nn.Module] = []\n",
        "        self.param_mappings: Dict[int, List[tuple]] = {}\n",
        "        self._max_pool_size = 8\n",
        "        \n",
        "    def get_layer_metadata(self) -> List[Dict[str, Any]]:\n",
        "        layers = []\n",
        "        layers.append({'id': 0, 'name': 'embeddings', 'type': 'embedding'})\n",
        "        for i in range(self.hf_config.num_hidden_layers):\n",
        "            layers.append({'id': i + 1, 'name': f'layers.{i}', 'type': 'block', 'block_index': i})\n",
        "        layers.append({'id': self.hf_config.num_hidden_layers + 1, 'name': 'head', 'type': 'head'})\n",
        "        return layers\n",
        "\n",
        "    def get_param_names_for_layer(self, layer_id: int) -> List[str]:\n",
        "        if layer_id == 0:\n",
        "            return ['model.embed_tokens.weight']\n",
        "        elif 1 <= layer_id <= self.hf_config.num_hidden_layers:\n",
        "            idx = layer_id - 1\n",
        "            prefix = f\"model.layers.{idx}\"\n",
        "            return [\n",
        "                f\"{prefix}.input_layernorm.weight\",\n",
        "                f\"{prefix}.self_attn.q_proj.weight\", f\"{prefix}.self_attn.k_proj.weight\",\n",
        "                f\"{prefix}.self_attn.v_proj.weight\", f\"{prefix}.self_attn.o_proj.weight\",\n",
        "                f\"{prefix}.post_attention_layernorm.weight\",\n",
        "                f\"{prefix}.mlp.gate_proj.weight\", f\"{prefix}.mlp.up_proj.weight\",\n",
        "                f\"{prefix}.mlp.down_proj.weight\",\n",
        "            ]\n",
        "        elif layer_id == self.hf_config.num_hidden_layers + 1:\n",
        "            return ['model.norm.weight', 'lm_head.weight']\n",
        "        return []\n",
        "\n",
        "    def construct_layer_module(self, layer_id: int, flat_buffer: Optional[torch.Tensor] = None, lora_manager: Optional[Any] = None) -> nn.Module:\n",
        "        if flat_buffer is not None:\n",
        "            device = flat_buffer.device\n",
        "        else:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        module = None\n",
        "        for i, m in enumerate(self.layer_pool):\n",
        "            if layer_id == 0 and isinstance(m, LlamaEmbeddingsLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif layer_id == self.hf_config.num_hidden_layers + 1 and isinstance(m, LlamaHeadLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif 1 <= layer_id <= self.hf_config.num_hidden_layers and isinstance(m, LlamaDecoderLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "        \n",
        "        if module is None:\n",
        "            if layer_id == 0: module = LlamaEmbeddingsLayer(self.hf_config, None)\n",
        "            elif layer_id == self.hf_config.num_hidden_layers + 1: module = LlamaHeadLayer(self.hf_config, None)\n",
        "            else:\n",
        "                module = LlamaDecoderLayer(self.hf_config, layer_idx=0)\n",
        "            \n",
        "            # Initialization only: move to target device\n",
        "            module.to(device)\n",
        "\n",
        "        if lora_manager and 1 <= layer_id <= self.hf_config.num_hidden_layers:\n",
        "            lora_manager.update_lora_params(layer_id, module)\n",
        "\n",
        "        if id(module) not in self.param_mappings:\n",
        "            self.param_mappings[id(module)] = self._create_mapping(layer_id, module)\n",
        "\n",
        "        if flat_buffer is not None:\n",
        "            mapping = self.param_mappings[id(module)]\n",
        "            offset = 0\n",
        "            with torch.no_grad():\n",
        "                for param, numel, shape in mapping:\n",
        "                    param.data.copy_(flat_buffer[offset : offset + numel].view(shape), non_blocking=True)\n",
        "                    offset += numel\n",
        "            \n",
        "        if hasattr(module, \"layer_idx\") and 1 <= layer_id <= self.hf_config.num_hidden_layers:\n",
        "            module.layer_idx = layer_id - 1\n",
        "        return module\n",
        "\n",
        "    def _create_mapping(self, layer_id: int, module: nn.Module) -> List[tuple]:\n",
        "        names = self.get_param_names_for_layer(layer_id)\n",
        "        idx = layer_id - 1\n",
        "        module_params = dict(module.named_parameters())\n",
        "        mapping = []\n",
        "        for full_name in names:\n",
        "            if layer_id == 0: clean_k = \"embed_tokens.weight\"\n",
        "            elif layer_id == self.hf_config.num_hidden_layers + 1:\n",
        "                clean_k = \"norm.weight\" if \"model.norm\" in full_name else \"lm_head.weight\"\n",
        "            else:\n",
        "                prefix = f\"model.layers.{idx}.\"\n",
        "                clean_k = full_name[len(prefix):]\n",
        "                if clean_k not in module_params: clean_k = clean_k.replace(\".weight\", \".base_layer.weight\")\n",
        "            param = module_params[clean_k]\n",
        "            mapping.append((param, param.numel(), param.shape))\n",
        "        return mapping\n",
        "\n",
        "    def release_layer_module(self, module: nn.Module):\n",
        "        if len(self.layer_pool) < self._max_pool_size:\n",
        "            self.layer_pool.append(module)\n",
        "        else:\n",
        "            del module\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "    def forward_layer(self, layer_module: nn.Module, inputs: Any, **kwargs) -> Any:\n",
        "        hidden_states = inputs[0] if isinstance(inputs, tuple) else inputs\n",
        "        \n",
        "        if isinstance(layer_module, LlamaDecoderLayer):\n",
        "            batch_size, seq_len = hidden_states.shape[:2]\n",
        "            device = hidden_states.device\n",
        "            position_ids = kwargs.get(\"position_ids\")\n",
        "            if position_ids is None:\n",
        "                position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "            \n",
        "            # Compute RoPE (Should be [bs, seq, dim])\n",
        "            attn = layer_module.self_attn\n",
        "            try:\n",
        "                if hasattr(attn, \"rotary_emb\") and attn.rotary_emb is not None:\n",
        "                    try: cos, sin = attn.rotary_emb(hidden_states, position_ids)\n",
        "                    except: cos, sin = attn.rotary_emb(position_ids)\n",
        "                else:\n",
        "                    cos, sin = self.rotary_emb(hidden_states, position_ids)\n",
        "            except Exception:\n",
        "                head_dim = self.hf_config.hidden_size // self.hf_config.num_attention_heads\n",
        "                dummy = torch.zeros(batch_size, seq_len, head_dim, device=device)\n",
        "                cos, sin = self.rotary_emb(dummy, position_ids)\n",
        "\n",
        "            # Ensure 3D [bs, seq, dim] for transformers broadcasting\n",
        "            if cos.ndim == 2:\n",
        "                cos = cos.unsqueeze(0)\n",
        "                sin = sin.unsqueeze(0)\n",
        "            elif cos.ndim == 4:\n",
        "                cos = cos.squeeze(1)\n",
        "                sin = sin.squeeze(1)\n",
        "            \n",
        "            # If batch size mismatch (e.g. rotary_emb returned [1, seq, dim] but bs > 1)\n",
        "            if cos.shape[0] != batch_size and cos.shape[0] == 1:\n",
        "                cos = cos.expand(batch_size, -1, -1)\n",
        "                sin = sin.expand(batch_size, -1, -1)\n",
        "\n",
        "            attention_mask = kwargs.get(\"attention_mask\")\n",
        "            if attention_mask is None:\n",
        "                mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=device)\n",
        "                mask = torch.triu(mask, diagonal=1)\n",
        "                attention_mask = mask.view(1, 1, seq_len, seq_len).expand(batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "            # 2. Manual Forward with Checkpointing\n",
        "            residual = hidden_states\n",
        "            hidden_states = layer_module.input_layernorm(hidden_states)\n",
        "            \n",
        "            # Wrap Self-Attention for memory efficiency\n",
        "            def attn_block(x, mask, pids, cos_sin):\n",
        "                return layer_module.self_attn(\n",
        "                    hidden_states=x,\n",
        "                    attention_mask=mask,\n",
        "                    position_ids=pids,\n",
        "                    position_embeddings=cos_sin,\n",
        "                    past_key_value=kwargs.get(\"past_key_value\"),\n",
        "                    output_attentions=kwargs.get(\"output_attentions\", False),\n",
        "                    use_cache=kwargs.get(\"use_cache\", False),\n",
        "                    cache_position=kwargs.get(\"cache_position\")\n",
        "                )[0]\n",
        "\n",
        "            if torch.is_grad_enabled() and kwargs.get(\"gradient_checkpointing\", False):\n",
        "                from torch.utils.checkpoint import checkpoint\n",
        "                attn_output = checkpoint(attn_block, hidden_states, attention_mask, position_ids, (cos, sin), use_reentrant=False)\n",
        "            else:\n",
        "                attn_output = attn_block(hidden_states, attention_mask, position_ids, (cos, sin))\n",
        "            \n",
        "            hidden_states = residual + attn_output\n",
        "            \n",
        "            # MLP block\n",
        "            residual = hidden_states\n",
        "            hidden_states = layer_module.post_attention_layernorm(hidden_states)\n",
        "            hidden_states = layer_module.mlp(hidden_states)\n",
        "            hidden_states = residual + hidden_states\n",
        "            \n",
        "            return hidden_states\n",
        "                \n",
        "        return layer_module(hidden_states)\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.hf_config.hidden_size\n",
        "\n",
        "class LlamaEmbeddingsLayer(nn.Module):\n",
        "    def __init__(self, config, weights):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n",
        "    def forward(self, x): return self.embed_tokens(x)\n",
        "\n",
        "class LlamaHeadLayer(nn.Module):\n",
        "    def __init__(self, config, weights):\n",
        "        super().__init__()\n",
        "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    def forward(self, x): return self.lm_head(self.norm(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile tasks/fine_tune_llama_7b.py\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from src.lema.core.gbi import GlobalBinaryIndex\n",
        "from src.lema.models.llama import LlamaAdapter\n",
        "from src.lema.engine.trainer import LemaTrainer\n",
        "from src.lema.core.lora import LoRAManager\n",
        "from src.lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "MODEL_PATH = \"llama2_7b.safetensors\"\n",
        "\n",
        "# 1. Realistic Dataset: \"Concise Assistant\"\n",
        "TRAINING_DATA = [\n",
        "    \"What is photosynthesis? Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "    \"Who was Albert Einstein? Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
        "    \"What is the capital of France? The capital of France is Paris.\",\n",
        "    \"Explain gravity. Gravity is a natural phenomenon by which all things with mass or energy are brought toward one another.\",\n",
        "    \"What is LEMA? LEMA is a framework that virtualizes GPU memory to enable training large models on limited hardware.\",\n",
        "    \"How does a CPU work? A CPU executes instructions of a computer program by performing basic arithmetic, logic, and I/O operations.\",\n",
        "    \"What is the speed of light? The speed of light in a vacuum is approximately 299,792,458 meters per second.\",\n",
        "    \"Define machine learning. Machine learning is a field of artificial intelligence focused on building systems that learn from data.\",\n",
        "    \"What is DNA? DNA is a molecule that carries the genetic instructions used in the growth, development, and functioning of all living organisms.\",\n",
        "    \"What is the ocean? The ocean is a continuous body of salt water that covers more than 70 percent of Earth's surface.\",\n",
        "] * 5 # 50 examples to keep it relatively fast for a demo\n",
        "\n",
        "def fine_tune_llama_7b_task():\n",
        "    print(\"--- STARTING LEMA 7B FINE-TUNING ---\")\n",
        "    \n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Config\n",
        "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    hf_config_dict = config.to_dict()\n",
        "    hf_config_dict[\"attn_implementation\"] = \"eager\"\n",
        "    hf_config_dict[\"torch_dtype\"] = \"float16\" # Ensure we use float16 config\n",
        "    \n",
        "    adapter = LlamaAdapter(hf_config_dict)\n",
        "    gbi = GlobalBinaryIndex(MODEL_PATH)\n",
        "    \n",
        "    # LEMA Config\n",
        "    lema_config = LemaConfig(\n",
        "        model_name_or_path=MODEL_PATH,\n",
        "        device=\"cuda\",\n",
        "        strategy=MemoryStrategy.STREAMING\n",
        "    )\n",
        "    \n",
        "    # LoRA Config\n",
        "    lora_config = {\n",
        "        \"r\": 16, \n",
        "        \"alpha\": 32, \n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    }\n",
        "    lora_manager = LoRAManager(lora_config, device=\"cuda\")\n",
        "    \n",
        "    print(\"Initializing LoRA parameters...\")\n",
        "    # Trigger param creation for all layers & release to avoid leak\n",
        "    for layer in adapter.get_layer_metadata():\n",
        "        if layer['type'] == 'block':\n",
        "            module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "            adapter.release_layer_module(module)\n",
        "            \n",
        "    torch.cuda.empty_cache()\n",
        "            \n",
        "    trainable_params = lora_manager.get_trainable_parameters()\n",
        "    print(f\"Number of trainable LoRA parameters: {len(trainable_params)}\")\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=5e-5)\n",
        "    \n",
        "    trainer = LemaTrainer(\n",
        "        config=lema_config,\n",
        "        model_adapter=adapter, \n",
        "        gbi=gbi, \n",
        "        lora_manager=lora_manager, \n",
        "        optimizer=optimizer\n",
        "    )\n",
        "    \n",
        "    # 3. Training Loop\n",
        "    print(f\"\\nTraining on {len(TRAINING_DATA)} examples...\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1): # 1 Epoch for demonstration\n",
        "        total_loss = 0\n",
        "        for i, text in enumerate(TRAINING_DATA):\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "            \n",
        "            logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "            total_loss += loss\n",
        "            \n",
        "            if (i+1) % 10 == 0:\n",
        "                print(f\"Step {i+1}/{len(TRAINING_DATA)} - Current Loss: {loss:.4f}\")\n",
        "                \n",
        "        avg_loss = total_loss / len(TRAINING_DATA)\n",
        "        print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds.\")\n",
        "        \n",
        "    # 4. Validation\n",
        "    print(\"\\n--- TESTING BEHAVIOR (Concise Assistant) ---\")\n",
        "    test_prompts = [\n",
        "        \"What is the moon?\",\n",
        "        \"Who was Isaac Newton?\"\n",
        "    ]\n",
        "    \n",
        "    for prompt in test_prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "        \n",
        "        generated = input_ids\n",
        "        for _ in range(25):\n",
        "            with torch.no_grad():\n",
        "                logits, _ = trainer.train_step(generated)\n",
        "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
        "            generated = torch.cat([generated, next_token_id], dim=-1)\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "                \n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {tokenizer.decode(generated[0], skip_special_tokens=True)}\")\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        os.remove(MODEL_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile tasks/fine_tune_llama_7b_config.py\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from src.lema.core.gbi import GlobalBinaryIndex\n",
        "from src.lema.models.llama import LlamaAdapter\n",
        "from src.lema.engine.trainer import LemaTrainer\n",
        "from src.lema.core.lora import LoRAManager\n",
        "from src.lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "MODEL_PATH = \"llama2_7b.safetensors\"\n",
        "\n",
        "TRAINING_DATA = [\n",
        "    \"What is photosynthesis? Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "    \"Who was Albert Einstein? Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
        "    \"What is the capital of France? The capital of France is Paris.\",\n",
        "    \"Explain gravity. Gravity is a natural phenomenon by which all things with mass or energy are brought toward one another.\",\n",
        "    \"What is LEMA? LEMA is a framework that virtualizes GPU memory to enable training large models on limited hardware.\",\n",
        "] * 10\n",
        "\n",
        "def fine_tune_llama_7b_with_config():\n",
        "    print(\"--- STARTING LEMA 7B FINE-TUNING (Config Object) ---\")\n",
        "    \n",
        "    # 1. Setup Configuration\n",
        "    config = LemaConfig(\n",
        "        model_name_or_path=MODEL_PATH,\n",
        "        device=\"cuda\",\n",
        "        strategy=MemoryStrategy.STREAMING,\n",
        "        lora_rank=16,\n",
        "        lora_alpha=32,\n",
        "        learning_rate=5e-5,\n",
        "        dtype=\"float16\"\n",
        "    )\n",
        "    \n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # HF Config (for model architecture)\n",
        "    hf_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    model_config = hf_config.to_dict()\n",
        "    model_config[\"attn_implementation\"] = config.attn_implementation\n",
        "    model_config[\"torch_dtype\"] = config.dtype\n",
        "    \n",
        "    # Components\n",
        "    adapter = LlamaAdapter(model_config)\n",
        "    gbi = GlobalBinaryIndex(config.gbi_path)\n",
        "    \n",
        "    # LoRA\n",
        "    lora_config_dict = {\n",
        "        \"r\": config.lora_rank, \n",
        "        \"alpha\": config.lora_alpha, \n",
        "        \"target_modules\": config.lora_target_modules\n",
        "    }\n",
        "    lora_manager = LoRAManager(lora_config_dict, device=config.device)\n",
        "    \n",
        "    print(\"Initializing LoRA parameters...\")\n",
        "    for layer in adapter.get_layer_metadata():\n",
        "        if layer['type'] == 'block':\n",
        "            module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "            adapter.release_layer_module(module)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    trainable_params = lora_manager.get_trainable_parameters()\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=config.learning_rate)\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = LemaTrainer(\n",
        "        config=config,\n",
        "        model_adapter=adapter, \n",
        "        gbi=gbi, \n",
        "        lora_manager=lora_manager, \n",
        "        optimizer=optimizer\n",
        "    )\n",
        "    \n",
        "    # Training Loop\n",
        "    print(f\"\\nTraining on {len(TRAINING_DATA)} examples...\")\n",
        "    for epoch in range(1):\n",
        "        total_loss = 0\n",
        "        for i, text in enumerate(TRAINING_DATA):\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            input_ids = inputs[\"input_ids\"].to(config.device)\n",
        "            \n",
        "            logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "            total_loss += loss\n",
        "            \n",
        "            if (i+1) % 10 == 0:\n",
        "                print(f\"Step {i+1}/{len(TRAINING_DATA)} - Current Loss: {loss:.4f}\")\n",
        "                \n",
        "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(TRAINING_DATA):.4f}\")\n",
        "    \n",
        "    print(\"Fine-tuning with Config Object completed successfully.\")\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        os.remove(MODEL_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile tasks/fine_tune_smollm.py\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from src.lema.core.gbi import GlobalBinaryIndex\n",
        "from src.lema.models.llama import LlamaAdapter\n",
        "from src.lema.engine.trainer import LemaTrainer\n",
        "from src.lema.core.lora import LoRAManager\n",
        "from src.lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
        "MODEL_PATH = \"smollm2_1.7b.safetensors\"\n",
        "\n",
        "# 1. Realistic Dataset: \"Concise Assistant\"\n",
        "# The model should learn to answer everything in one short, professional sentence.\n",
        "TRAINING_DATA = [\n",
        "    \"What is photosynthesis? Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "    \"Who was Albert Einstein? Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
        "    \"What is the capital of France? The capital of France is Paris.\",\n",
        "    \"Explain gravity. Gravity is a natural phenomenon by which all things with mass or energy are brought toward one another.\",\n",
        "    \"What is LEMA? LEMA is a framework that virtualizes GPU memory to enable training large models on limited hardware.\",\n",
        "    \"How does a CPU work? A CPU executes instructions of a computer program by performing basic arithmetic, logic, and I/O operations.\",\n",
        "    \"What is the speed of light? The speed of light in a vacuum is approximately 299,792,458 meters per second.\",\n",
        "    \"Define machine learning. Machine learning is a field of artificial intelligence focused on building systems that learn from data.\",\n",
        "    \"What is DNA? DNA is a molecule that carries the genetic instructions used in the growth, development, and functioning of all living organisms.\",\n",
        "    \"What is the ocean? The ocean is a continuous body of salt water that covers more than 70 percent of Earth's surface.\",\n",
        "] * 10 # 100 examples for more \"realistic\" weight updates\n",
        "\n",
        "def fine_tune_realistic():\n",
        "    print(\"--- STARTING REALISTIC LEMA FINE-TUNING (v0.6) ---\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    hf_config_dict = config.to_dict()\n",
        "    hf_config_dict[\"attn_implementation\"] = \"eager\"\n",
        "    \n",
        "    adapter = LlamaAdapter(hf_config_dict)\n",
        "    gbi = GlobalBinaryIndex(MODEL_PATH)\n",
        "    \n",
        "    # LEMA Config\n",
        "    lema_config = LemaConfig(\n",
        "        model_name_or_path=MODEL_PATH,\n",
        "        device=\"cuda\",\n",
        "        strategy=MemoryStrategy.STREAMING\n",
        "    )\n",
        "    \n",
        "    # 2. HEAVY LoRA Config (All major linear layers)\n",
        "    # This increases the number of parameters the optimizer has to manage.\n",
        "    lora_config = {\n",
        "        \"r\": 16, \n",
        "        \"alpha\": 32, \n",
        "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    }\n",
        "    lora_manager = LoRAManager(lora_config, device=\"cuda\")\n",
        "    \n",
        "    print(\"Initializing heavy LoRA parameters...\")\n",
        "    # Trigger param creation for all layers\n",
        "    for layer in adapter.get_layer_metadata():\n",
        "        if layer['type'] == 'block':\n",
        "            module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "            adapter.release_layer_module(module)\n",
        "    \n",
        "    # Clear cache after initialization\n",
        "    torch.cuda.empty_cache()\n",
        "            \n",
        "    trainable_params = lora_manager.get_trainable_parameters()\n",
        "    print(f\"Number of trainable LoRA parameters: {len(trainable_params)}\")\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=5e-5)\n",
        "    \n",
        "    trainer = LemaTrainer(\n",
        "        config=lema_config,\n",
        "        model_adapter=adapter, \n",
        "        gbi=gbi, \n",
        "        lora_manager=lora_manager, \n",
        "        optimizer=optimizer\n",
        "    )\n",
        "    \n",
        "    # 3. Training Loop\n",
        "    print(\"\\nTraining on 100 examples...\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(3): # Fewer epochs but more data per epoch\n",
        "        total_loss = 0\n",
        "        for i, text in enumerate(TRAINING_DATA):\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "            \n",
        "            logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "            total_loss += loss\n",
        "            \n",
        "            if (i+1) % 20 == 0:\n",
        "                print(f\"Step {i+1}/{len(TRAINING_DATA)} - Current Loss: {loss:.4f}\")\n",
        "                \n",
        "        avg_loss = total_loss / len(TRAINING_DATA)\n",
        "        print(f\"Epoch {epoch+1}/3 - Avg Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds.\")\n",
        "        \n",
        "    # 4. Validation\n",
        "    print(\"\\n--- TESTING BEHAVIOR (Concise Assistant) ---\")\n",
        "    test_prompts = [\n",
        "        \"What is the moon?\",\n",
        "        \"Who was Isaac Newton?\"\n",
        "    ]\n",
        "    \n",
        "    for prompt in test_prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "        \n",
        "        generated = input_ids\n",
        "        for _ in range(25):\n",
        "            with torch.no_grad():\n",
        "                logits, _ = trainer.train_step(generated)\n",
        "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(-1)\n",
        "            generated = torch.cat([generated, next_token_id], dim=-1)\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "                \n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Response: {tokenizer.decode(generated[0], skip_special_tokens=True)}\")\n",
        "\n",
        "    # Final Cleanup\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        os.remove(MODEL_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Expecting benchmark script to have downloaded the model already\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        fine_tune_realistic()\n",
        "    else:\n",
        "        print(f\"Error: {MODEL_PATH} not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('src'))\n",
        "print('LEMA library loaded.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q safetensors accelerate peft transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "# LEMA Imports\n",
        "from src.lema.core.gbi import GlobalBinaryIndex\n",
        "from src.lema.models.llama import LlamaAdapter\n",
        "from src.lema.engine.trainer import LemaTrainer\n",
        "from src.lema.core.lora import LoRAManager\n",
        "from src.lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "print(f\"Using Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# --- MODELS TO TEST ---\n",
        "# Focusing on Llama architectures for speed comparison\n",
        "MODELS = [\n",
        "    {\n",
        "        \"name\": \"TinyLlama 1.1B\",\n",
        "        \"hf_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"path\": \"tinyllama_1b.safetensors\",\n",
        "        \"type\": \"llama\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Llama-2 7B\",\n",
        "        \"hf_id\": \"NousResearch/Llama-2-7b-hf\",\n",
        "        \"path\": \"llama2_7b.safetensors\",\n",
        "        \"type\": \"llama\"\n",
        "    }\n",
        "]\n",
        "\n",
        "NUM_STEPS = 20 # Enough to stabilize avg speed\n",
        "\n",
        "def download_and_convert(model_info):\n",
        "    print(f\"\\n--- Preparing {model_info['name']} ---\")\n",
        "    if os.path.exists(model_info['path']):\n",
        "        print(f\"{model_info['path']} already exists.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading {model_info['hf_id']}...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_info['hf_id'], \n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"cpu\"\n",
        "    )\n",
        "    \n",
        "    # Break shared weights if necessary\n",
        "    if hasattr(model, \"lm_head\") and hasattr(model, \"model\") and hasattr(model.model, \"embed_tokens\"):\n",
        "         if model.lm_head.weight.data_ptr() == model.model.embed_tokens.weight.data_ptr():\n",
        "             model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "\n",
        "    print(f\"Saving to {model_info['path']}...\")\n",
        "    save_file(model.state_dict(), model_info['path'])\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "def benchmark_peft_speed(model_info):\n",
        "    print(f\"\\n>>> BENCHMARKING PEFT SPEED: {model_info['name']} <<<\")\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_info['hf_id'],\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"cuda\"\n",
        "        )\n",
        "        \n",
        "        # Enable Gradient Checkpointing to save memory\n",
        "        model.gradient_checkpointing_enable()\n",
        "        \n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "        peft_config = LoraConfig(\n",
        "            r=16, lora_alpha=32, target_modules=target_modules,\n",
        "            lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        model = get_peft_model(model, peft_config)\n",
        "        \n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "        input_ids = torch.randint(0, 1000, (1, 512)).cuda()\n",
        "        \n",
        "        # Warmup\n",
        "        model(input_ids, labels=input_ids).loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        torch.cuda.synchronize()\n",
        "        \n",
        "        print(f\"Running {NUM_STEPS} steps...\")\n",
        "        start_time = time.time()\n",
        "        for _ in range(NUM_STEPS):\n",
        "            loss = model(input_ids, labels=input_ids).loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        \n",
        "        avg_time = (end_time - start_time) / NUM_STEPS\n",
        "        print(f\"PEFT Avg Time/Step: {avg_time:.4f}s\")\n",
        "        \n",
        "        del model\n",
        "        del optimizer\n",
        "        torch.cuda.empty_cache()\n",
        "        return avg_time\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"PEFT Benchmark Failed: {e}\")\n",
        "        return float('inf')\n",
        "\n",
        "def benchmark_lema_speed(model_info):\n",
        "    print(f\"\\n>>> BENCHMARKING LEMA SPEED: {model_info['name']} <<<\")\n",
        "    torch.cuda.empty_cache()\n",
        "    download_and_convert(model_info)\n",
        "    \n",
        "    # Enable checkpointing only for large models (e.g. 7B)\n",
        "    use_gc = \"7b\" in model_info['hf_id'].lower()\n",
        "    print(f\"Gradient Checkpointing: {use_gc}\")\n",
        "    \n",
        "    try:\n",
        "        hf_config = AutoConfig.from_pretrained(model_info['hf_id'])\n",
        "        hf_config_dict = hf_config.to_dict()\n",
        "        hf_config_dict[\"attn_implementation\"] = \"eager\"\n",
        "        hf_config_dict[\"torch_dtype\"] = \"float16\"\n",
        "        \n",
        "        adapter = LlamaAdapter(hf_config_dict)\n",
        "        gbi = GlobalBinaryIndex(model_info['path'])\n",
        "        \n",
        "        lema_config = LemaConfig(\n",
        "            model_name_or_path=model_info['path'],\n",
        "            device=\"cuda\",\n",
        "            strategy=MemoryStrategy.STREAMING,\n",
        "            learning_rate=1e-4,\n",
        "            gradient_checkpointing=use_gc\n",
        "        )\n",
        "        \n",
        "        lora_config = {\n",
        "            \"r\": 16, \"alpha\": 32, \n",
        "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "        }\n",
        "        lora_manager = LoRAManager(lora_config, device=\"cuda\")\n",
        "        \n",
        "        # Init params\n",
        "        for layer in adapter.get_layer_metadata():\n",
        "            if layer['type'] == 'block':\n",
        "                module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "                adapter.release_layer_module(module)\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        trainable_params = lora_manager.get_trainable_parameters()\n",
        "        optimizer = torch.optim.AdamW(trainable_params, lr=lema_config.learning_rate)\n",
        "        \n",
        "        trainer = LemaTrainer(\n",
        "            config=lema_config,\n",
        "            model_adapter=adapter, \n",
        "            gbi=gbi, \n",
        "            lora_manager=lora_manager, \n",
        "            optimizer=optimizer\n",
        "        )\n",
        "        \n",
        "        input_ids = torch.randint(0, 1000, (1, 512)).cuda()\n",
        "        \n",
        "        # Warmup\n",
        "        trainer.train_step(input_ids, labels=input_ids)\n",
        "        torch.cuda.synchronize()\n",
        "        \n",
        "        print(f\"Running {NUM_STEPS} steps...\")\n",
        "        start_time = time.time()\n",
        "        for _ in range(NUM_STEPS):\n",
        "            trainer.train_step(input_ids, labels=input_ids)\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        \n",
        "        avg_time = (end_time - start_time) / NUM_STEPS\n",
        "        print(f\"LEMA Avg Time/Step: {avg_time:.4f}s\")\n",
        "        \n",
        "        return avg_time\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"LEMA Benchmark Failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return float('inf')\n",
        "    finally:\n",
        "        if os.path.exists(model_info['path']):\n",
        "            os.remove(model_info['path'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = {}\n",
        "    for model in MODELS:\n",
        "        peft_time = benchmark_peft_speed(model)\n",
        "        lema_time = benchmark_lema_speed(model)\n",
        "        results[model['name']] = {\"PEFT\": peft_time, \"LEMA\": lema_time}\n",
        "    \n",
        "    print(\"\\n=== SPEED BENCHMARK RESULTS (Time per Step) ===\")\n",
        "    print(f\"{ 'Model':<20} | { 'PEFT (s)':<10} | { 'LEMA (s)':<10} | { 'Overhead':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "    for name, data in results.items():\n",
        "        peft = data[\"PEFT\"]\n",
        "        lema = data[\"LEMA\"]\n",
        "        overhead = (lema / peft) if peft > 0 else float('inf')\n",
        "        print(f\"{name:<20} | {peft:<10.4f} | {lema:<10.4f} | {overhead:<10.2f}x\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}