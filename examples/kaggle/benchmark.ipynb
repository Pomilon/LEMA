{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEMA: Layer-wise Efficient Memory Abstraction\n",
        "This notebook is a self-contained LEMA workspace. It includes the library, examples, tasks, and documentation.\n",
        "\n",
        "--- \n",
        "# LEMA: Layer-wise Efficient Memory Abstraction\n",
        "\n",
        "**Virtualize GPU VRAM for LLM Fine-Tuning**\n",
        "\n",
        "LEMA is a specialized framework designed to facilitate the fine-tuning of Large Language Models (LLMs) on hardware where model size exceeds available VRAM. By treating model weights as addressable binary segments and implementing a **Triple-Buffer Strategy**, LEMA allows training 7B+ models on GPUs with as little as 16GB VRAM.\n",
        "\n",
        "## Key Performance (Tesla P100 - 16GB)\n",
        "\n",
        "| Model | Standard PEFT | LEMA | Status |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Llama-2 7B** | **OOM (Crash)** | **5.90 GB VRAM** | **Stable** |\n",
        "| **SmolLM2 1.7B**| 3.88 GB | 3.20 GB | Stable |\n",
        "| **TinyLlama 1.1B**| 2.67 GB | 2.12 GB | Stable |\n",
        "\n",
        "## Core Features\n",
        "\n",
        "- **Global Binary Index (GBI)**: Zero-copy mapping of `.safetensors` files using `mmap`.\n",
        "- **Triple-Buffer Pipeline**: Pipelined data movement (Disk -> RAM -> VRAM) to hide PCIe latency.\n",
        "- **High-Level API**: Simplified `LemaModel` and `LemaTrainer` interfaces for fast integration.\n",
        "- **Automatic Checkpointing**: Built-in interval-based saving of LoRA adapters and optimizer states.\n",
        "\n",
        "## Installation\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/Pomilon/LEMA.git\n",
        "cd LEMA\n",
        "pip install -e .\n",
        "```\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from lema import LemaConfig, LemaModel, MemoryStrategy\n",
        "\n",
        "# 1. Configuration\n",
        "config = LemaConfig(\n",
        "    model_name_or_path=\"NousResearch/Llama-2-7b-hf\",\n",
        "    gbi_path=\"llama2_7b.safetensors\", # Single monolithic safetensors file\n",
        "    strategy=MemoryStrategy.STREAMING,\n",
        "    lora_rank=16,\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "# 2. Initialize Model & Trainer\n",
        "model = LemaModel(config)\n",
        "model.initialize_lora() # Pre-initialize adapters\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=1e-4)\n",
        "trainer = model.get_trainer(optimizer)\n",
        "\n",
        "# 3. Train\n",
        "input_ids = torch.randint(0, 32000, (1, 512)).cuda()\n",
        "logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "```\n",
        "\n",
        "## Documentation\n",
        "\n",
        "- [**User Guide**](docs/USER_GUIDE.md): Model preparation, conversion, and tips.\n",
        "- [**API Reference**](docs/API_REFERENCE.md): Detailed class and method specifications.\n",
        "- [**Architecture**](docs/ARCHITECTURE.md): Deep dive into the memory pipeline and LEMA-loop.\n",
        "\n",
        "## Kaggle Benchmark\n",
        "\n",
        "You can run the latest verification suite on Kaggle using the provided notebook:\n",
        "[**LEMA Benchmark Notebook**](https://www.kaggle.com/code/kloyford/lema-benchmark-notebook)\n",
        "\n",
        "## License\n",
        "MIT License - Copyright (c) 2026 Pomilon\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!mkdir -p src/lema/core src/lema/engine src/lema/models src/lema/utils tasks examples/kaggle docs output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/config.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional, Dict, Any\n",
        "from enum import Enum\n",
        "\n",
        "class MemoryStrategy(Enum):\n",
        "    STREAMING = \"streaming\" # Disk -> RAM -> VRAM\n",
        "    RESIDENT = \"resident\"   # RAM -> VRAM (No Disk offload for weights)\n",
        "\n",
        "@dataclass\n",
        "class LemaConfig:\n",
        "    \"\"\"\n",
        "    Central Configuration for LEMA Training/Inference.\n",
        "    \"\"\"\n",
        "    # Model Settings\n",
        "    model_name_or_path: str\n",
        "    model_type: Optional[str] = None # 'llama' or 'gpt2', auto-detected if None\n",
        "    gbi_path: Optional[str] = None # Path to converted safetensors for GBI\n",
        "    \n",
        "    # Hardware / Memory Settings\n",
        "    device: str = \"cuda\"\n",
        "    strategy: MemoryStrategy = MemoryStrategy.STREAMING\n",
        "    ram_buffer_size: int = 2 # Number of layers to keep in RAM\n",
        "    vram_buffer_size: int = 1 # Number of layers to keep in VRAM\n",
        "    \n",
        "    # LoRA Settings\n",
        "    use_lora: bool = True\n",
        "    lora_rank: int = 16\n",
        "    lora_alpha: int = 32\n",
        "    lora_target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
        "    \n",
        "    # Training Settings\n",
        "    learning_rate: float = 1e-4\n",
        "    batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    max_seq_length: int = 512\n",
        "    gradient_checkpointing: bool = False\n",
        "    \n",
        "    # Checkpointing Settings\n",
        "    save_steps: int = 500\n",
        "    output_dir: str = \"output\"\n",
        "    \n",
        "    # Advanced\n",
        "    dtype: str = \"float16\" # float16, bfloat16, float32\n",
        "    attn_implementation: str = \"eager\" # eager, sdpa, flash_attention_2\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.gbi_path is None:\n",
        "            # Default to expecting a local safetensors file named after the model or a standard name\n",
        "            if self.model_name_or_path.endswith(\".safetensors\"):\n",
        "                self.gbi_path = self.model_name_or_path\n",
        "            else:\n",
        "                self.gbi_path = \"model.safetensors\"\n",
        "        \n",
        "        if isinstance(self.strategy, str):\n",
        "            self.strategy = MemoryStrategy(self.strategy.lower())\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            k: v.value if isinstance(v, Enum) else v \n",
        "            for k, v in self.__dict__.items()\n",
        "        }\n",
        "\n",
        "    def save_pretrained(self, save_directory: str):\n",
        "        import os\n",
        "        import json\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "        config_file = os.path.join(save_directory, \"lema_config.json\")\n",
        "        with open(config_file, \"w\") as f:\n",
        "            json.dump(self.to_dict(), f, indent=4)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, load_directory: str, **kwargs):\n",
        "        import os\n",
        "        import json\n",
        "        config_file = os.path.join(load_directory, \"lema_config.json\")\n",
        "        if not os.path.exists(config_file):\n",
        "            raise FileNotFoundError(f\"Config file not found in {load_directory}\")\n",
        "        \n",
        "        with open(config_file, \"r\") as f:\n",
        "            config_dict = json.load(f)\n",
        "        \n",
        "        # Override with kwargs\n",
        "        config_dict.update(kwargs)\n",
        "        \n",
        "        # Handle enum conversion\n",
        "        if \"strategy\" in config_dict and isinstance(config_dict[\"strategy\"], str):\n",
        "            config_dict[\"strategy\"] = MemoryStrategy(config_dict[\"strategy\"])\n",
        "            \n",
        "        return cls(**config_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/__init__.py\n",
        "from .config import LemaConfig, MemoryStrategy\n",
        "from .core.model import LemaModel\n",
        "from .engine.trainer import LemaTrainer\n",
        "\n",
        "__version__ = \"0.1.1\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/utils/model_utils.py\n",
        "import torch\n",
        "import os\n",
        "from safetensors.torch import save_file\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "def break_shared_weights(model: torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Ensures that shared weights (like lm_head and embed_tokens) are distinct copies.\n",
        "    Required for safetensors compatibility.\n",
        "    \"\"\"\n",
        "    if hasattr(model, \"lm_head\") and hasattr(model, \"model\") and hasattr(model.model, \"embed_tokens\"):\n",
        "        if model.lm_head.weight.data_ptr() == model.model.embed_tokens.weight.data_ptr():\n",
        "            # Only clone the specific shared tensor, not the whole model\n",
        "            model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "    return model\n",
        "\n",
        "def prepare_monolithic_safetensors(model_name_or_path: str, output_path: str, device: str = \"auto\"):\n",
        "    \"\"\"\n",
        "    Downloads a model and saves it as a single, framework-compatible safetensors file.\n",
        "    Uses 'auto' device map to offload to GPU and save System RAM during conversion.\n",
        "    \"\"\"\n",
        "    print(f\"Loading {model_name_or_path} for monolithic conversion...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=device\n",
        "    )\n",
        "    model = break_shared_weights(model)\n",
        "    \n",
        "    print(f\"Saving monolithic safetensors to {output_path}...\")\n",
        "    # Pass state_dict directly to save_file to avoid memory doubling\n",
        "    sd = model.state_dict()\n",
        "    save_file(sd, output_path)\n",
        "    \n",
        "    # Cleanup\n",
        "    del sd\n",
        "    del model\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/engine/trainer.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import threading\n",
        "import os\n",
        "from typing import Any, Optional, List, Union\n",
        "from ..core.memory import TripleBufferManager\n",
        "from ..models.base import LemaModelAdapter\n",
        "from ..config import LemaConfig, MemoryStrategy\n",
        "\n",
        "class LemaTrainer:\n",
        "    def __init__(self, \n",
        "                 config: LemaConfig,\n",
        "                 model_adapter: LemaModelAdapter, \n",
        "                 gbi: Any, \n",
        "                 lora_manager: Any = None, \n",
        "                 optimizer: Optional[torch.optim.Optimizer] = None,\n",
        "                 memory_manager: Optional[TripleBufferManager] = None):\n",
        "        \n",
        "        self.config = config\n",
        "        self.adapter = model_adapter\n",
        "        self.gbi = gbi\n",
        "        self.device = config.device\n",
        "        self.strategy = config.strategy\n",
        "        \n",
        "        # Use provided memory manager or create a new one\n",
        "        if memory_manager is not None:\n",
        "            self.memory = memory_manager\n",
        "        else:\n",
        "            self.memory = TripleBufferManager(gbi, model_adapter, self.device, strategy=self.strategy)\n",
        "        \n",
        "        self.layers = self.adapter.get_layer_metadata()\n",
        "        self.lora_manager = lora_manager\n",
        "        self.optimizer = optimizer\n",
        "        self.global_step = 0\n",
        "\n",
        "    def save_checkpoint(self, save_directory: str):\n",
        "        \"\"\"Saves the model state (config + LoRA) and optionally optimizer state.\"\"\"\n",
        "        self.config.save_pretrained(save_directory)\n",
        "        if self.lora_manager:\n",
        "            self.lora_manager.save_pretrained(save_directory)\n",
        "        \n",
        "        if self.optimizer:\n",
        "            torch.save(self.optimizer.state_dict(), os.path.join(save_directory, \"optimizer.bin\"))\n",
        "\n",
        "    def train_step(self, inputs: Any, labels: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Executes one forward pass and one backward pass.\n",
        "        If labels are provided, computes CrossEntropyLoss.\n",
        "        \"\"\"\n",
        "        boundary_activations: List[torch.Tensor] = []\n",
        "        is_streaming = (self.strategy == MemoryStrategy.STREAMING)\n",
        "        \n",
        "        # --- FORWARD PASS ---\n",
        "        if is_streaming:\n",
        "            self.memory.prefetch_to_ram(self.layers[0]['id'], 0)\n",
        "            self.memory.async_transfer_to_vram(self.layers[0]['id'], 0, ram_slot=0)\n",
        "            if len(self.layers) > 1:\n",
        "                self.memory.prefetch_to_ram(self.layers[1]['id'], 1)\n",
        "        else:\n",
        "            self.memory.async_transfer_to_vram(self.layers[0]['id'], 0)\n",
        "\n",
        "        hidden_states = inputs\n",
        "        \n",
        "        for i, layer_meta in enumerate(self.layers):\n",
        "            slot = i % 2\n",
        "            next_slot = (i + 1) % 2\n",
        "            \n",
        "            flat_vram = self.memory.get_vram_flat_buffer(slot)\n",
        "            \n",
        "            disk_thread = None\n",
        "            if i + 1 < len(self.layers):\n",
        "                if is_streaming:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i+1]['id'], next_slot, ram_slot=next_slot)\n",
        "                    if i + 2 < len(self.layers):\n",
        "                        disk_thread = threading.Thread(target=self.memory.prefetch_to_ram, args=(self.layers[i+2]['id'], slot))\n",
        "                        disk_thread.start()\n",
        "                else:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i+1]['id'], next_slot)\n",
        "            \n",
        "            layer_module = self.adapter.construct_layer_module(layer_meta['id'], flat_vram, self.lora_manager)\n",
        "            \n",
        "            # Store input for backward\n",
        "            if isinstance(hidden_states, tuple): \n",
        "                 current_input = hidden_states[0].detach()\n",
        "            else:\n",
        "                current_input = hidden_states.detach()\n",
        "            boundary_activations.append(current_input)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                hidden_states = self.adapter.forward_layer(layer_module, hidden_states, gradient_checkpointing=False)\n",
        "\n",
        "            if disk_thread: disk_thread.join()\n",
        "            if hasattr(self.adapter, \"release_layer_module\"):\n",
        "                self.adapter.release_layer_module(layer_module)\n",
        "            del layer_module\n",
        "\n",
        "        # Final Logits\n",
        "        logits = hidden_states\n",
        "        loss_val = None\n",
        "\n",
        "        # --- BACKWARD PASS ---\n",
        "        if not torch.is_grad_enabled():\n",
        "            return logits, None\n",
        "\n",
        "        last_idx = len(self.layers) - 1\n",
        "        if is_streaming:\n",
        "            self.memory.prefetch_to_ram(self.layers[last_idx]['id'], 0)\n",
        "            self.memory.async_transfer_to_vram(self.layers[last_idx]['id'], 0, ram_slot=0)\n",
        "            if last_idx > 0:\n",
        "                self.memory.prefetch_to_ram(self.layers[last_idx-1]['id'], 1)\n",
        "        else:\n",
        "            self.memory.async_transfer_to_vram(self.layers[last_idx]['id'], 0)\n",
        "        \n",
        "        grad_output = None\n",
        "        \n",
        "        for i in range(last_idx, -1, -1):\n",
        "            slot = (last_idx - i) % 2\n",
        "            next_slot = (last_idx - i + 1) % 2\n",
        "            \n",
        "            flat_vram = self.memory.get_vram_flat_buffer(slot)\n",
        "            \n",
        "            disk_thread = None\n",
        "            if i - 1 >= 0:\n",
        "                if is_streaming:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i-1]['id'], next_slot, ram_slot=next_slot)\n",
        "                    if i - 2 >= 0:\n",
        "                        disk_thread = threading.Thread(target=self.memory.prefetch_to_ram, args=(self.layers[i-2]['id'], slot))\n",
        "                        disk_thread.start()\n",
        "                else:\n",
        "                    self.memory.async_transfer_to_vram(self.layers[i-1]['id'], next_slot)\n",
        "            \n",
        "            layer_module = self.adapter.construct_layer_module(self.layers[i]['id'], flat_vram, self.lora_manager)\n",
        "            layer_input = boundary_activations[i]\n",
        "            if layer_input.dtype.is_floating_point:\n",
        "                layer_input.requires_grad_(True)\n",
        "            \n",
        "            output = self.adapter.forward_layer(layer_module, layer_input, gradient_checkpointing=self.config.gradient_checkpointing)\n",
        "            \n",
        "            if i == last_idx:\n",
        "                if labels is not None:\n",
        "                    # Real Causal LM Loss\n",
        "                    # Shift so that tokens < n predict n\n",
        "                    shift_logits = output[..., :-1, :].contiguous()\n",
        "                    shift_labels = labels[..., 1:].contiguous()\n",
        "                    loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "                    loss_val = loss.item()\n",
        "                else:\n",
        "                    loss = output.mean() # Dummy\n",
        "                \n",
        "                loss.backward()\n",
        "                grad_output = layer_input.grad\n",
        "            else:\n",
        "                if isinstance(output, tuple):\n",
        "                    output[0].backward(grad_output)\n",
        "                else:\n",
        "                    output.backward(grad_output)\n",
        "                grad_output = layer_input.grad\n",
        "            \n",
        "            if disk_thread: disk_thread.join()\n",
        "            if hasattr(self.adapter, \"release_layer_module\"):\n",
        "                self.adapter.release_layer_module(layer_module)\n",
        "            del layer_module\n",
        "\n",
        "        if self.optimizer:\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "        self.global_step += 1\n",
        "        \n",
        "        # Automatic checkpointing\n",
        "        if self.config.save_steps > 0 and self.global_step % self.config.save_steps == 0:\n",
        "            checkpoint_path = os.path.join(self.config.output_dir, f\"checkpoint-{self.global_step}\")\n",
        "            self.save_checkpoint(checkpoint_path)\n",
        "\n",
        "        return logits, loss_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/lora.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "try:\n",
        "    from transformers.pytorch_utils import Conv1D\n",
        "except ImportError:\n",
        "    Conv1D = None\n",
        "\n",
        "class LoRAWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps a Linear or Conv1D layer with LoRA adapters.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_layer: nn.Module, rank: int, alpha: float, lora_A: nn.Parameter, lora_B: nn.Parameter):\n",
        "        super().__init__()\n",
        "        self.base_layer = base_layer\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "        self.scaling = alpha / rank\n",
        "        \n",
        "        self.lora_A = lora_A\n",
        "        self.lora_B = lora_B\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Base forward\n",
        "        result = self.base_layer(x)\n",
        "        \n",
        "        # LoRA forward\n",
        "        # Calculation: (x @ A.T @ B.T) * scaling\n",
        "        lora_out = (x @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
        "        return result + lora_out\n",
        "\n",
        "class LoRAManager:\n",
        "    \"\"\"\n",
        "    Manages the lifecycle and storage of LoRA parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict, device=\"cuda\"):\n",
        "        self.rank = config.get(\"r\", 8)\n",
        "        self.alpha = config.get(\"alpha\", 16)\n",
        "        self.target_modules = config.get(\"target_modules\", [\"c_attn\", \"c_proj\", \"c_fc\"])\n",
        "        self.device = device\n",
        "        \n",
        "        # Store parameters: key -> {'A': Param, 'B': Param}\n",
        "        self.params: Dict[str, Dict[str, nn.Parameter]] = {}\n",
        "        \n",
        "    def get_or_init_params(self, layer_id: int, module_name: str, in_features: int, out_features: int) -> Dict[str, nn.Parameter]:\n",
        "        key = f\"{layer_id}.{module_name}\"\n",
        "        \n",
        "        if key not in self.params:\n",
        "            lora_A = torch.zeros((self.rank, in_features), device=self.device)\n",
        "            nn.init.kaiming_uniform_(lora_A, a=math.sqrt(5))\n",
        "            \n",
        "            lora_B = torch.zeros((out_features, self.rank), device=self.device)\n",
        "            nn.init.zeros_(lora_B)\n",
        "            \n",
        "            self.params[key] = {\n",
        "                'A': nn.Parameter(lora_A, requires_grad=True),\n",
        "                'B': nn.Parameter(lora_B, requires_grad=True)\n",
        "            }\n",
        "            \n",
        "        return self.params[key]\n",
        "\n",
        "    def apply_lora(self, layer_id: int, module: nn.Module, module_name_prefix: str = \"\"):\n",
        "        \"\"\"\n",
        "        Recursively replaces Linear/Conv1D layers with LoRAWrapper if they match target_modules.\n",
        "        \"\"\"\n",
        "        for name, child in module.named_children():\n",
        "            full_name = f\"{module_name_prefix}.{name}\" if module_name_prefix else name\n",
        "            \n",
        "            # Check if this is a target module\n",
        "            is_target = any(name == target or name.endswith(target) for target in self.target_modules)\n",
        "            \n",
        "            if isinstance(child, LoRAWrapper) and is_target:\n",
        "                # Already wrapped, just swap parameters for the new layer\n",
        "                if isinstance(child.base_layer, nn.Linear):\n",
        "                    in_features = child.base_layer.in_features\n",
        "                    out_features = child.base_layer.out_features\n",
        "                elif Conv1D is not None and isinstance(child.base_layer, Conv1D):\n",
        "                    in_features = child.base_layer.weight.shape[0]\n",
        "                    out_features = child.base_layer.weight.shape[1]\n",
        "                else:\n",
        "                    # Generic fallback if weight exists\n",
        "                    in_features = child.base_layer.weight.shape[1] if hasattr(child.base_layer, \"weight\") else 0\n",
        "                    out_features = child.base_layer.weight.shape[0] if hasattr(child.base_layer, \"weight\") else 0\n",
        "\n",
        "                params = self.get_or_init_params(layer_id, full_name, in_features, out_features)\n",
        "                child.lora_A = params['A']\n",
        "                child.lora_B = params['B']\n",
        "                continue\n",
        "\n",
        "            in_features = None\n",
        "            out_features = None\n",
        "            \n",
        "            if isinstance(child, nn.Linear) and is_target:\n",
        "                in_features = child.in_features\n",
        "                out_features = child.out_features\n",
        "            elif Conv1D is not None and isinstance(child, Conv1D) and is_target:\n",
        "                in_features = child.weight.shape[0]\n",
        "                out_features = child.weight.shape[1]\n",
        "                \n",
        "            if in_features is not None and out_features is not None:\n",
        "                params = self.get_or_init_params(layer_id, full_name, in_features, out_features)\n",
        "                \n",
        "                lora_layer = LoRAWrapper(\n",
        "                    base_layer=child,\n",
        "                    rank=self.rank,\n",
        "                    alpha=self.alpha,\n",
        "                    lora_A=params['A'],\n",
        "                    lora_B=params['B']\n",
        "                )\n",
        "                setattr(module, name, lora_layer)\n",
        "            else:\n",
        "                self.apply_lora(layer_id, child, full_name)\n",
        "\n",
        "    def update_lora_params(self, layer_id: int, module: nn.Module):\n",
        "        \"\"\"\n",
        "        Efficiently updates LoRA parameters for a reused module.\n",
        "        Uses cached wrapper list if available, otherwise traverses and builds cache.\n",
        "        \"\"\"\n",
        "        if not hasattr(module, \"_lora_cache\"):\n",
        "            module._lora_cache = []\n",
        "            # First time: Traverse and collect wrappers\n",
        "            # We reuse apply_lora logic but adapted for collection\n",
        "            self._collect_and_update_wrappers(layer_id, module, \"\", module._lora_cache)\n",
        "        else:\n",
        "            # Fast path: Update parameters from cache\n",
        "            for wrapper, name, in_f, out_f in module._lora_cache:\n",
        "                params = self.get_or_init_params(layer_id, name, in_f, out_f)\n",
        "                wrapper.lora_A = params['A']\n",
        "                wrapper.lora_B = params['B']\n",
        "\n",
        "    def _collect_and_update_wrappers(self, layer_id: int, module: nn.Module, prefix: str, cache: List):\n",
        "        for name, child in module.named_children():\n",
        "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "            \n",
        "            if isinstance(child, LoRAWrapper):\n",
        "                # Already wrapped (from previous usage or just now)\n",
        "                in_f = child.base_layer.in_features if hasattr(child.base_layer, \"in_features\") else child.base_layer.weight.shape[1]\n",
        "                out_f = child.base_layer.out_features if hasattr(child.base_layer, \"out_features\") else child.base_layer.weight.shape[0]\n",
        "                \n",
        "                params = self.get_or_init_params(layer_id, full_name, in_f, out_f)\n",
        "                child.lora_A = params['A']\n",
        "                child.lora_B = params['B']\n",
        "                \n",
        "                cache.append((child, full_name, in_f, out_f))\n",
        "                continue\n",
        "            \n",
        "            # Check if this is a target module to wrap\n",
        "            is_target = any(name == target or name.endswith(target) for target in self.target_modules)\n",
        "            \n",
        "            if is_target and (isinstance(child, nn.Linear) or (Conv1D is not None and isinstance(child, Conv1D))):\n",
        "                in_features = child.in_features if isinstance(child, nn.Linear) else child.weight.shape[0]\n",
        "                out_features = child.out_features if isinstance(child, nn.Linear) else child.weight.shape[1]\n",
        "                \n",
        "                params = self.get_or_init_params(layer_id, full_name, in_features, out_features)\n",
        "                \n",
        "                lora_layer = LoRAWrapper(\n",
        "                    base_layer=child,\n",
        "                    rank=self.rank,\n",
        "                    alpha=self.alpha,\n",
        "                    lora_A=params['A'],\n",
        "                    lora_B=params['B']\n",
        "                )\n",
        "                setattr(module, name, lora_layer)\n",
        "                cache.append((lora_layer, full_name, in_features, out_features))\n",
        "            else:\n",
        "                self._collect_and_update_wrappers(layer_id, child, full_name, cache)\n",
        "\n",
        "    def get_trainable_parameters(self) -> List[torch.nn.Parameter]:\n",
        "        \"\"\"\n",
        "        Returns a list of all nn.Parameter objects managed by this manager.\n",
        "        \"\"\"\n",
        "        all_params = []\n",
        "        for p_dict in self.params.values():\n",
        "            all_params.append(p_dict['A'])\n",
        "            all_params.append(p_dict['B'])\n",
        "        return all_params\n",
        "\n",
        "    def save_pretrained(self, save_directory: str):\n",
        "        import os\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "        # Filter for only LoRA weights\n",
        "        state_dict = {}\n",
        "        for key, p_dict in self.params.items():\n",
        "            state_dict[f\"{key}.lora_A\"] = p_dict['A'].data.cpu()\n",
        "            state_dict[f\"{key}.lora_B\"] = p_dict['B'].data.cpu()\n",
        "        \n",
        "        torch.save(state_dict, os.path.join(save_directory, \"adapter_model.bin\"))\n",
        "\n",
        "    def load_pretrained(self, load_directory: str):\n",
        "        import os\n",
        "        weight_path = os.path.join(load_directory, \"adapter_model.bin\")\n",
        "        if not os.path.exists(weight_path):\n",
        "            raise FileNotFoundError(f\"Adapter weights not found in {load_directory}\")\n",
        "        \n",
        "        state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
        "        for full_key, tensor in state_dict.items():\n",
        "            # full_key is e.g. \"1.self_attn.q_proj.lora_A\"\n",
        "            parts = full_key.split(\".\")\n",
        "            param_type = parts[-1] # lora_A or lora_B\n",
        "            key = \".\".join(parts[:-1]) # e.g. \"1.self_attn.q_proj\"\n",
        "            \n",
        "            if key not in self.params:\n",
        "                self.params[key] = {}\n",
        "            \n",
        "            p_dict = self.params[key]\n",
        "            if param_type == \"lora_A\":\n",
        "                if 'A' not in p_dict:\n",
        "                    p_dict['A'] = nn.Parameter(tensor.to(self.device), requires_grad=True)\n",
        "                else:\n",
        "                    p_dict['A'].data.copy_(tensor.to(self.device))\n",
        "            elif param_type == \"lora_B\":\n",
        "                if 'B' not in p_dict:\n",
        "                    p_dict['B'] = nn.Parameter(tensor.to(self.device), requires_grad=True)\n",
        "                else:\n",
        "                    p_dict['B'].data.copy_(tensor.to(self.device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/model.py\n",
        "import torch\n",
        "import os\n",
        "from typing import Optional, Dict, Any, Union\n",
        "from transformers import AutoConfig\n",
        "\n",
        "from ..config import LemaConfig\n",
        "from ..models import get_adapter\n",
        "from .gbi import GlobalBinaryIndex\n",
        "from .lora import LoRAManager\n",
        "from .memory import TripleBufferManager\n",
        "\n",
        "class LemaModel:\n",
        "    \"\"\"\n",
        "    High-level interface for LEMA Models.\n",
        "    Wraps all low-level components into a single object.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: LemaConfig):\n",
        "        self.config = config\n",
        "        \n",
        "        # 1. Initialize GBI\n",
        "        self.gbi = GlobalBinaryIndex(config.gbi_path)\n",
        "        \n",
        "        # 2. Get HF config for the adapter\n",
        "        # Try to load from model path or fallback to a default if not found\n",
        "        try:\n",
        "            hf_config_obj = AutoConfig.from_pretrained(config.model_name_or_path)\n",
        "            hf_config_dict = hf_config_obj.to_dict()\n",
        "        except:\n",
        "            # Fallback to config dict if AutoConfig fails\n",
        "            hf_config_dict = config.to_dict()\n",
        "\n",
        "        # 3. Initialize Adapter\n",
        "        model_type = config.model_type\n",
        "        if model_type is None:\n",
        "            # Auto-detect from path\n",
        "            path_lower = config.model_name_or_path.lower()\n",
        "            if \"llama\" in path_lower or \"smollm\" in path_lower:\n",
        "                model_type = \"llama\"\n",
        "            elif \"gpt2\" in path_lower:\n",
        "                model_type = \"gpt2\"\n",
        "            else:\n",
        "                # Default to llama if unknown but looks like it\n",
        "                model_type = \"llama\"\n",
        "        \n",
        "        self.adapter = get_adapter(model_type, hf_config_dict)\n",
        "        \n",
        "        # 4. Initialize LoRA Manager\n",
        "        self.lora_manager = LoRAManager({\n",
        "            \"r\": config.lora_rank,\n",
        "            \"alpha\": config.lora_alpha,\n",
        "            \"target_modules\": config.lora_target_modules\n",
        "        }, device=config.device)\n",
        "        \n",
        "        # 5. Initialize Memory Manager\n",
        "        self.memory = TripleBufferManager(\n",
        "            self.gbi, \n",
        "            self.adapter, \n",
        "            device=config.device, \n",
        "            strategy=config.strategy\n",
        "        )\n",
        "\n",
        "    def get_trainer(self, optimizer: torch.optim.Optimizer):\n",
        "        \"\"\"Returns a LemaTrainer instance pre-configured with this model's components.\"\"\"\n",
        "        from ..engine.trainer import LemaTrainer\n",
        "        return LemaTrainer(\n",
        "            config=self.config,\n",
        "            model_adapter=self.adapter,\n",
        "            gbi=self.gbi,\n",
        "            lora_manager=self.lora_manager,\n",
        "            optimizer=optimizer,\n",
        "            memory_manager=self.memory\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, path: str, **kwargs):\n",
        "        \"\"\"Loads a LEMA model and its adapters from a directory.\"\"\"\n",
        "        config = LemaConfig.from_pretrained(path, **kwargs)\n",
        "        model = cls(config)\n",
        "        \n",
        "        # Load adapters if they exist\n",
        "        if os.path.exists(os.path.join(path, \"adapter_model.bin\")):\n",
        "            model.lora_manager.load_pretrained(path)\n",
        "            \n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory: str):\n",
        "        \"\"\"Saves the configuration and LoRA adapters.\"\"\"\n",
        "        self.config.save_pretrained(save_directory)\n",
        "        self.lora_manager.save_pretrained(save_directory)\n",
        "\n",
        "    def initialize_lora(self):\n",
        "        \"\"\"Pre-initializes all LoRA adapters by constructing and releasing each layer once.\"\"\"\n",
        "        for layer in self.adapter.get_layer_metadata():\n",
        "            if layer['type'] == 'block':\n",
        "                module = self.adapter.construct_layer_module(layer['id'], None, self.lora_manager)\n",
        "                if hasattr(self.adapter, \"release_layer_module\"):\n",
        "                    self.adapter.release_layer_module(module)\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        return self.lora_manager.get_trainable_parameters()\n",
        "\n",
        "    def to(self, device: str):\n",
        "        self.config.device = device\n",
        "        self.lora_manager.device = device\n",
        "        self.memory.device = device\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/gbi.py\n",
        "import torch\n",
        "from safetensors import safe_open\n",
        "from typing import Dict, List, Any, Optional\n",
        "import os\n",
        "\n",
        "class GlobalBinaryIndex:\n",
        "    \"\"\"\n",
        "    GBI v0.4: Contiguous Block Access.\n",
        "    Allows fetching a whole layer as a single byte-range if possible,\n",
        "    but here we focus on providing tensors for contiguous packing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "        \n",
        "        self.model_path = model_path\n",
        "        self.handle = safe_open(self.model_path, framework=\"pt\", device=\"cpu\")\n",
        "        self.keys = list(self.handle.keys())\n",
        "\n",
        "    def load_tensors(self, param_names: List[str], device: str = \"cpu\") -> Dict[str, torch.Tensor]:\n",
        "        tensors = {}\n",
        "        for name in param_names:\n",
        "            tensors[name] = self.handle.get_tensor(name)\n",
        "        return tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/core/memory.py\n",
        "import torch\n",
        "import threading\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "from enum import Enum\n",
        "import gc\n",
        "from ..config import MemoryStrategy\n",
        "\n",
        "class TripleBufferManager:\n",
        "    \"\"\"\n",
        "    Unified Memory Manager supporting both Disk-Streaming and RAM-Residency.\n",
        "    \"\"\"\n",
        "    def __init__(self, gbi, adapter, device=\"cuda\", strategy=MemoryStrategy.STREAMING):\n",
        "        self.gbi = gbi\n",
        "        self.adapter = adapter\n",
        "        self.device = device\n",
        "        self.strategy = strategy\n",
        "        \n",
        "        self.is_cuda = self.device.startswith(\"cuda\")\n",
        "        self.transfer_streams = [torch.cuda.Stream() for _ in range(2)] if self.is_cuda else None\n",
        "        \n",
        "        self.layers_meta = self.adapter.get_layer_metadata()\n",
        "        \n",
        "        # Calculate max layer size for pre-allocating buffers\n",
        "        self.max_params = self._calculate_max_params()\n",
        "        \n",
        "        # Pre-allocated VRAM slots (Double buffering)\n",
        "        self.vram_flat_buffers = [\n",
        "            torch.empty(self.max_params, device=self.device, dtype=torch.float32)\n",
        "            for _ in range(2)\n",
        "        ]\n",
        "        \n",
        "        # RAM Buffers\n",
        "        if self.strategy == MemoryStrategy.RESIDENT:\n",
        "            print(f\"LEMA: Initializing RESIDENT strategy (Caching model in RAM)...\")\n",
        "            self.ram_flat_buffers: Dict[int, torch.Tensor] = {}\n",
        "            self._initialize_full_ram_cache()\n",
        "        else:\n",
        "            print(f\"LEMA: Initializing STREAMING strategy (Default)...\")\n",
        "            # In streaming mode, we only need 2 RAM slots for the pipeline\n",
        "            self.ram_flat_buffers: List[torch.Tensor] = [\n",
        "                torch.empty(self.max_params, device=\"cpu\", dtype=torch.float32).pin_memory() if self.is_cuda else torch.empty(self.max_params, device=\"cpu\", dtype=torch.float32)\n",
        "                for _ in range(2)\n",
        "            ]\n",
        "            self.ram_layer_ids = [-1, -1]\n",
        "\n",
        "    def _calculate_max_params(self) -> int:\n",
        "        max_p = 0\n",
        "        for layer in self.layers_meta:\n",
        "            names = self.adapter.get_param_names_for_layer(layer['id'])\n",
        "            current_p = 0\n",
        "            for name in names:\n",
        "                meta = self.gbi.handle.get_slice(name)\n",
        "                current_p += meta.get_shape().numel() if hasattr(meta.get_shape(), 'numel') else torch.Size(meta.get_shape()).numel()\n",
        "            max_p = max(max_p, current_p)\n",
        "        return max_p\n",
        "\n",
        "    def _initialize_full_ram_cache(self):\n",
        "        \"\"\"Pre-packs the entire model into pinned RAM.\"\"\"\n",
        "        for layer in self.layers_meta:\n",
        "            layer_id = layer['id']\n",
        "            self._pack_layer_to_ram(layer_id, is_resident=True)\n",
        "\n",
        "    def _pack_layer_to_ram(self, layer_id: int, slot: int = 0, is_resident: bool = False):\n",
        "        \"\"\"Helper to load a layer from disk and pack it into a flat RAM buffer.\"\"\"\n",
        "        param_names = self.adapter.get_param_names_for_layer(layer_id)\n",
        "        weights = self.gbi.load_tensors(param_names, device=\"cpu\")\n",
        "        \n",
        "        if is_resident:\n",
        "            total_el = sum(w.numel() for w in weights.values())\n",
        "            buf = torch.empty(total_el, device=\"cpu\", dtype=torch.float32).pin_memory()\n",
        "        else:\n",
        "            buf = self.ram_flat_buffers[slot]\n",
        "            \n",
        "        offset = 0\n",
        "        for name in param_names:\n",
        "            w = weights[name]\n",
        "            numel = w.numel()\n",
        "            buf[offset : offset + numel].copy_(w.view(-1))\n",
        "            offset += numel\n",
        "            \n",
        "        if is_resident:\n",
        "            self.ram_flat_buffers[layer_id] = buf\n",
        "        else:\n",
        "            self.ram_layer_ids[slot] = layer_id\n",
        "\n",
        "    def prefetch_to_ram(self, layer_id: int, ram_slot: int):\n",
        "        \"\"\"Stage 1 (Streaming only): Load from Disk to RAM Slot.\"\"\"\n",
        "        if self.strategy == MemoryStrategy.RESIDENT:\n",
        "            return # No-op for resident mode\n",
        "            \n",
        "        if self.ram_layer_ids[ram_slot] == layer_id:\n",
        "            return\n",
        "        \n",
        "        self._pack_layer_to_ram(layer_id, ram_slot, is_resident=False)\n",
        "\n",
        "    def async_transfer_to_vram(self, layer_id: int, vram_slot: int, ram_slot: Optional[int] = None):\n",
        "        \"\"\"Stage 2: Async transfer to GPU.\"\"\"\n",
        "        if self.strategy == MemoryStrategy.RESIDENT:\n",
        "            src_buf = self.ram_flat_buffers[layer_id]\n",
        "        else:\n",
        "            if ram_slot is None:\n",
        "                raise ValueError(\"ram_slot must be provided in streaming mode\")\n",
        "            src_buf = self.ram_flat_buffers[ram_slot]\n",
        "            \n",
        "        vram_dest = self.vram_flat_buffers[vram_slot]\n",
        "        \n",
        "        if self.is_cuda and self.transfer_streams:\n",
        "            stream = self.transfer_streams[vram_slot]\n",
        "            with torch.cuda.stream(stream):\n",
        "                vram_dest[:src_buf.numel()].copy_(src_buf, non_blocking=True)\n",
        "        else:\n",
        "            # CPU or Synchronous copy\n",
        "            vram_dest[:src_buf.numel()].copy_(src_buf)\n",
        "\n",
        "    def get_vram_flat_buffer(self, vram_slot: int) -> torch.Tensor:\n",
        "        \"\"\"Stage 3: Usage.\"\"\"\n",
        "        if self.is_cuda and self.transfer_streams:\n",
        "            self.transfer_streams[vram_slot].synchronize()\n",
        "        return self.vram_flat_buffers[vram_slot]\n",
        "\n",
        "    def clear_vram_slot(self, vram_slot: int):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/base.py\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LemaModelAdapter(ABC):\n",
        "    \"\"\"\n",
        "    Abstract Base Class for LEMA Model Adapters.\n",
        "    \n",
        "    This class defines the interface that any model architecture must implement\n",
        "    to be compatible with the LEMA (Layer-wise Efficient Memory Abstraction) framework.\n",
        "    It bridges the gap between the raw binary weights managed by LEMA and the \n",
        "    PyTorch execution semantics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_layer_metadata(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Returns a list of dictionaries, where each dictionary describes a logical \"layer\"\n",
        "        or \"block\" in the model that LEMA should manage as a unit.\n",
        "        \n",
        "        Returns:\n",
        "            List[Dict]: e.g. [{'id': 0, 'name': 'transformer.h.0', 'inputs': [...], 'outputs': [...]}, ...]\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def construct_layer_module(self, layer_id: int, weights: Dict[str, torch.Tensor], lora_manager: Optional[Any] = None) -> nn.Module:\n",
        "        \"\"\"\n",
        "        Constructs a PyTorch nn.Module for the specified layer using the provided weights.\n",
        "        The weights will be on the target device (VRAM) when passed here.\n",
        "        \n",
        "        Args:\n",
        "            layer_id (int): The index of the layer to construct.\n",
        "            weights (Dict[str, torch.Tensor]): A dictionary mapping parameter names to tensors.\n",
        "            lora_manager (Optional[Any]): The LoRAManager instance to apply adapters.\n",
        "            \n",
        "        Returns:\n",
        "            nn.Module: The executable layer module.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward_layer(self, layer_module: nn.Module, inputs: Any, **kwargs) -> Any:\n",
        "        \"\"\"\n",
        "        Executes the forward pass for a single layer.\n",
        "        \n",
        "        Args:\n",
        "            layer_module (nn.Module): The module constructed by construct_layer_module.\n",
        "            inputs (Any): The input activations (tensor or tuple of tensors).\n",
        "            **kwargs: Additional arguments (e.g., attention masks, rotary embeddings).\n",
        "            \n",
        "        Returns:\n",
        "            Any: The output activations.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_param_names_for_layer(self, layer_id: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Returns the list of parameter names (as found in the safetensors file) \n",
        "        required for the specified layer.\n",
        "        \n",
        "        Args:\n",
        "            layer_id (int): Layer index.\n",
        "            \n",
        "        Returns:\n",
        "            List[str]: List of parameter keys.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def hidden_size(self) -> int:\n",
        "        \"\"\"Returns the model's hidden size for buffer allocation.\"\"\"\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/__init__.py\n",
        "from .base import LemaModelAdapter\n",
        "from .llama import LlamaAdapter\n",
        "from .gpt2 import GPT2Adapter\n",
        "\n",
        "_ADAPTER_REGISTRY = {\n",
        "    \"llama\": LlamaAdapter,\n",
        "    \"gpt2\": GPT2Adapter\n",
        "}\n",
        "\n",
        "def get_adapter(model_type: str, config: dict) -> LemaModelAdapter:\n",
        "    if model_type not in _ADAPTER_REGISTRY:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}. Available: {list(_ADAPTER_REGISTRY.keys())}\")\n",
        "    return _ADAPTER_REGISTRY[model_type](config)\n",
        "\n",
        "def register_adapter(model_type: str, adapter_class: type):\n",
        "    _ADAPTER_REGISTRY[model_type] = adapter_class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/gpt2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Config\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from .base import LemaModelAdapter\n",
        "\n",
        "class GPT2Adapter(LemaModelAdapter):\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__(config)\n",
        "        self.hf_config = GPT2Config(**config)\n",
        "        if getattr(self.hf_config, \"_attn_implementation\", None) is None:\n",
        "            self.hf_config._attn_implementation = config.get(\"attn_implementation\", \"eager\")\n",
        "        self.layer_pool: List[nn.Module] = []\n",
        "        self.param_mappings: Dict[int, List[tuple]] = {}\n",
        "        \n",
        "    def get_layer_metadata(self) -> List[Dict[str, Any]]:\n",
        "        layers = []\n",
        "        layers.append({'id': 0, 'name': 'embeddings', 'type': 'embedding'})\n",
        "        for i in range(self.hf_config.n_layer):\n",
        "            layers.append({'id': i + 1, 'name': f'h.{i}', 'type': 'block', 'block_index': i})\n",
        "        layers.append({'id': self.hf_config.n_layer + 1, 'name': 'head', 'type': 'head'})\n",
        "        return layers\n",
        "\n",
        "    def get_param_names_for_layer(self, layer_id: int) -> List[str]:\n",
        "        if layer_id == 0:\n",
        "            return ['transformer.wte.weight', 'transformer.wpe.weight']\n",
        "        elif 1 <= layer_id <= self.hf_config.n_layer:\n",
        "            idx = layer_id - 1\n",
        "            prefix = f\"transformer.h.{idx}\"\n",
        "            return [\n",
        "                f\"{prefix}.attn.c_attn.weight\", f\"{prefix}.attn.c_attn.bias\",\n",
        "                f\"{prefix}.attn.c_proj.weight\", f\"{prefix}.attn.c_proj.bias\",\n",
        "                f\"{prefix}.ln_1.weight\", f\"{prefix}.ln_1.bias\",\n",
        "                f\"{prefix}.ln_2.weight\", f\"{prefix}.ln_2.bias\",\n",
        "                f\"{prefix}.mlp.c_fc.weight\", f\"{prefix}.mlp.c_fc.bias\",\n",
        "                f\"{prefix}.mlp.c_proj.weight\", f\"{prefix}.mlp.c_proj.bias\",\n",
        "            ]\n",
        "        elif layer_id == self.hf_config.n_layer + 1:\n",
        "            return ['transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight']\n",
        "        return []\n",
        "\n",
        "    def construct_layer_module(self, layer_id: int, flat_buffer: Optional[torch.Tensor] = None, lora_manager: Optional[Any] = None) -> nn.Module:\n",
        "        device = flat_buffer.device if flat_buffer is not None else torch.device(\"cpu\")\n",
        "        module = None\n",
        "        for i, m in enumerate(self.layer_pool):\n",
        "            if layer_id == 0 and isinstance(m, GPT2EmbeddingsLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif layer_id == self.hf_config.n_layer + 1 and isinstance(m, GPT2HeadLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif 1 <= layer_id <= self.hf_config.n_layer and isinstance(m, GPT2Block):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "        \n",
        "        if module is None:\n",
        "            if layer_id == 0: module = GPT2EmbeddingsLayer(self.hf_config)\n",
        "            elif layer_id == self.hf_config.n_layer + 1: module = GPT2HeadLayer(self.hf_config)\n",
        "            else:\n",
        "                module = GPT2Block(self.hf_config)\n",
        "            \n",
        "            # Initialization only\n",
        "            module.to(device)\n",
        "\n",
        "        if lora_manager and 1 <= layer_id <= self.hf_config.n_layer:\n",
        "            lora_manager.update_lora_params(layer_id, module)\n",
        "\n",
        "        if id(module) not in self.param_mappings:\n",
        "            self.param_mappings[id(module)] = self._create_mapping(layer_id, module)\n",
        "\n",
        "        if flat_buffer is not None:\n",
        "            mapping = self.param_mappings[id(module)]\n",
        "            offset = 0\n",
        "            with torch.no_grad():\n",
        "                for param, numel, shape in mapping:\n",
        "                    param.data.copy_(flat_buffer[offset : offset + numel].view(shape), non_blocking=True)\n",
        "                    offset += numel\n",
        "            \n",
        "        return module\n",
        "\n",
        "    def _create_mapping(self, layer_id: int, module: nn.Module) -> List[tuple]:\n",
        "        names = self.get_param_names_for_layer(layer_id)\n",
        "        idx = layer_id - 1\n",
        "        module_params = dict(module.named_parameters())\n",
        "        mapping = []\n",
        "        for full_name in names:\n",
        "            if layer_id == 0:\n",
        "                clean_k = \"wte.weight\" if \"wte\" in full_name else \"wpe.weight\"\n",
        "            elif layer_id == self.hf_config.n_layer + 1:\n",
        "                if \"ln_f\" in full_name: clean_k = \"ln_f.weight\" if \"weight\" in full_name else \"ln_f.bias\"\n",
        "                else: clean_k = \"head.weight\"\n",
        "            else:\n",
        "                prefix = f\"transformer.h.{idx}.\"\n",
        "                clean_k = full_name[len(prefix):]\n",
        "                if clean_k not in module_params: clean_k = clean_k.replace(\".weight\", \".base_layer.weight\").replace(\".bias\", \".base_layer.bias\")\n",
        "            param = module_params[clean_k]\n",
        "            mapping.append((param, param.numel(), param.shape))\n",
        "        return mapping\n",
        "\n",
        "    def release_layer_module(self, module: nn.Module):\n",
        "        if len(self.layer_pool) < 5:\n",
        "            self.layer_pool.append(module)\n",
        "\n",
        "    def forward_layer(self, layer_module: nn.Module, inputs: Any, **kwargs) -> Any:\n",
        "        hidden_states = inputs[0] if isinstance(inputs, tuple) else inputs\n",
        "        if isinstance(layer_module, GPT2Block):\n",
        "            return layer_module(hidden_states)[0]\n",
        "        return layer_module(hidden_states)\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.hf_config.n_embd\n",
        "\n",
        "class GPT2EmbeddingsLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "    def forward(self, input_ids):\n",
        "        position_ids = torch.arange(0, input_ids.size(-1), dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "        return self.wte(input_ids) + self.wpe(position_ids)\n",
        "\n",
        "class GPT2HeadLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "    def forward(self, x): return self.head(self.ln_f(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile src/lema/models/llama.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm, LlamaConfig, LlamaRotaryEmbedding\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from .base import LemaModelAdapter\n",
        "\n",
        "class LlamaAdapter(LemaModelAdapter):\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        super().__init__(config)\n",
        "        self.hf_config = LlamaConfig(**config)\n",
        "        if getattr(self.hf_config, \"_attn_implementation\", None) is None:\n",
        "            self.hf_config._attn_implementation = config.get(\"attn_implementation\", \"eager\")\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(self.hf_config)\n",
        "        self.layer_pool: List[nn.Module] = []\n",
        "        self.param_mappings: Dict[int, List[tuple]] = {}\n",
        "        self._max_pool_size = 8\n",
        "        \n",
        "    def get_layer_metadata(self) -> List[Dict[str, Any]]:\n",
        "        layers = []\n",
        "        layers.append({'id': 0, 'name': 'embeddings', 'type': 'embedding'})\n",
        "        for i in range(self.hf_config.num_hidden_layers):\n",
        "            layers.append({'id': i + 1, 'name': f'layers.{i}', 'type': 'block', 'block_index': i})\n",
        "        layers.append({'id': self.hf_config.num_hidden_layers + 1, 'name': 'head', 'type': 'head'})\n",
        "        return layers\n",
        "\n",
        "    def get_param_names_for_layer(self, layer_id: int) -> List[str]:\n",
        "        if layer_id == 0:\n",
        "            return ['model.embed_tokens.weight']\n",
        "        elif 1 <= layer_id <= self.hf_config.num_hidden_layers:\n",
        "            idx = layer_id - 1\n",
        "            prefix = f\"model.layers.{idx}\"\n",
        "            return [\n",
        "                f\"{prefix}.input_layernorm.weight\",\n",
        "                f\"{prefix}.self_attn.q_proj.weight\", f\"{prefix}.self_attn.k_proj.weight\",\n",
        "                f\"{prefix}.self_attn.v_proj.weight\", f\"{prefix}.self_attn.o_proj.weight\",\n",
        "                f\"{prefix}.post_attention_layernorm.weight\",\n",
        "                f\"{prefix}.mlp.gate_proj.weight\", f\"{prefix}.mlp.up_proj.weight\",\n",
        "                f\"{prefix}.mlp.down_proj.weight\",\n",
        "            ]\n",
        "        elif layer_id == self.hf_config.num_hidden_layers + 1:\n",
        "            return ['model.norm.weight', 'lm_head.weight']\n",
        "        return []\n",
        "\n",
        "    def construct_layer_module(self, layer_id: int, flat_buffer: Optional[torch.Tensor] = None, lora_manager: Optional[Any] = None) -> nn.Module:\n",
        "        if flat_buffer is not None:\n",
        "            device = flat_buffer.device\n",
        "        else:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        module = None\n",
        "        for i, m in enumerate(self.layer_pool):\n",
        "            if layer_id == 0 and isinstance(m, LlamaEmbeddingsLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif layer_id == self.hf_config.num_hidden_layers + 1 and isinstance(m, LlamaHeadLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "            elif 1 <= layer_id <= self.hf_config.num_hidden_layers and isinstance(m, LlamaDecoderLayer):\n",
        "                module = self.layer_pool.pop(i); break\n",
        "        \n",
        "        if module is None:\n",
        "            if layer_id == 0: module = LlamaEmbeddingsLayer(self.hf_config, None)\n",
        "            elif layer_id == self.hf_config.num_hidden_layers + 1: module = LlamaHeadLayer(self.hf_config, None)\n",
        "            else:\n",
        "                module = LlamaDecoderLayer(self.hf_config, layer_idx=0)\n",
        "            \n",
        "            # Initialization only: move to target device\n",
        "            module.to(device)\n",
        "\n",
        "        if lora_manager and 1 <= layer_id <= self.hf_config.num_hidden_layers:\n",
        "            lora_manager.update_lora_params(layer_id, module)\n",
        "\n",
        "        if id(module) not in self.param_mappings:\n",
        "            self.param_mappings[id(module)] = self._create_mapping(layer_id, module)\n",
        "\n",
        "        if flat_buffer is not None:\n",
        "            mapping = self.param_mappings[id(module)]\n",
        "            offset = 0\n",
        "            with torch.no_grad():\n",
        "                for param, numel, shape in mapping:\n",
        "                    param.data.copy_(flat_buffer[offset : offset + numel].view(shape), non_blocking=True)\n",
        "                    offset += numel\n",
        "            \n",
        "        if hasattr(module, \"layer_idx\") and 1 <= layer_id <= self.hf_config.num_hidden_layers:\n",
        "            module.layer_idx = layer_id - 1\n",
        "        return module\n",
        "\n",
        "    def _create_mapping(self, layer_id: int, module: nn.Module) -> List[tuple]:\n",
        "        names = self.get_param_names_for_layer(layer_id)\n",
        "        idx = layer_id - 1\n",
        "        module_params = dict(module.named_parameters())\n",
        "        mapping = []\n",
        "        for full_name in names:\n",
        "            if layer_id == 0: clean_k = \"embed_tokens.weight\"\n",
        "            elif layer_id == self.hf_config.num_hidden_layers + 1:\n",
        "                clean_k = \"norm.weight\" if \"model.norm\" in full_name else \"lm_head.weight\"\n",
        "            else:\n",
        "                prefix = f\"model.layers.{idx}.\"\n",
        "                clean_k = full_name[len(prefix):]\n",
        "                if clean_k not in module_params: clean_k = clean_k.replace(\".weight\", \".base_layer.weight\")\n",
        "            param = module_params[clean_k]\n",
        "            mapping.append((param, param.numel(), param.shape))\n",
        "        return mapping\n",
        "\n",
        "    def release_layer_module(self, module: nn.Module):\n",
        "        if len(self.layer_pool) < self._max_pool_size:\n",
        "            self.layer_pool.append(module)\n",
        "        else:\n",
        "            del module\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "    def forward_layer(self, layer_module: nn.Module, inputs: Any, **kwargs) -> Any:\n",
        "        hidden_states = inputs[0] if isinstance(inputs, tuple) else inputs\n",
        "        \n",
        "        if isinstance(layer_module, LlamaDecoderLayer):\n",
        "            batch_size, seq_len = hidden_states.shape[:2]\n",
        "            device = hidden_states.device\n",
        "            position_ids = kwargs.get(\"position_ids\")\n",
        "            if position_ids is None:\n",
        "                position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "            \n",
        "            # Compute RoPE (Should be [bs, seq, dim])\n",
        "            attn = layer_module.self_attn\n",
        "            try:\n",
        "                if hasattr(attn, \"rotary_emb\") and attn.rotary_emb is not None:\n",
        "                    try: cos, sin = attn.rotary_emb(hidden_states, position_ids)\n",
        "                    except: cos, sin = attn.rotary_emb(position_ids)\n",
        "                else:\n",
        "                    cos, sin = self.rotary_emb(hidden_states, position_ids)\n",
        "            except Exception:\n",
        "                head_dim = self.hf_config.hidden_size // self.hf_config.num_attention_heads\n",
        "                dummy = torch.zeros(batch_size, seq_len, head_dim, device=device)\n",
        "                cos, sin = self.rotary_emb(dummy, position_ids)\n",
        "\n",
        "            # Ensure 3D [bs, seq, dim] for transformers broadcasting\n",
        "            if cos.ndim == 2:\n",
        "                cos = cos.unsqueeze(0)\n",
        "                sin = sin.unsqueeze(0)\n",
        "            elif cos.ndim == 4:\n",
        "                cos = cos.squeeze(1)\n",
        "                sin = sin.squeeze(1)\n",
        "            \n",
        "            # If batch size mismatch (e.g. rotary_emb returned [1, seq, dim] but bs > 1)\n",
        "            if cos.shape[0] != batch_size and cos.shape[0] == 1:\n",
        "                cos = cos.expand(batch_size, -1, -1)\n",
        "                sin = sin.expand(batch_size, -1, -1)\n",
        "\n",
        "            attention_mask = kwargs.get(\"attention_mask\")\n",
        "            if attention_mask is None:\n",
        "                mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=device)\n",
        "                mask = torch.triu(mask, diagonal=1)\n",
        "                attention_mask = mask.view(1, 1, seq_len, seq_len).expand(batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "            # 2. Manual Forward with Checkpointing\n",
        "            residual = hidden_states\n",
        "            hidden_states = layer_module.input_layernorm(hidden_states)\n",
        "            \n",
        "            # Wrap Self-Attention for memory efficiency\n",
        "            def attn_block(x, mask, pids, cos_sin):\n",
        "                return layer_module.self_attn(\n",
        "                    hidden_states=x,\n",
        "                    attention_mask=mask,\n",
        "                    position_ids=pids,\n",
        "                    position_embeddings=cos_sin,\n",
        "                    past_key_value=kwargs.get(\"past_key_value\"),\n",
        "                    output_attentions=kwargs.get(\"output_attentions\", False),\n",
        "                    use_cache=kwargs.get(\"use_cache\", False),\n",
        "                    cache_position=kwargs.get(\"cache_position\")\n",
        "                )[0]\n",
        "\n",
        "            if torch.is_grad_enabled() and kwargs.get(\"gradient_checkpointing\", False):\n",
        "                from torch.utils.checkpoint import checkpoint\n",
        "                attn_output = checkpoint(attn_block, hidden_states, attention_mask, position_ids, (cos, sin), use_reentrant=False)\n",
        "            else:\n",
        "                attn_output = attn_block(hidden_states, attention_mask, position_ids, (cos, sin))\n",
        "            \n",
        "            hidden_states = residual + attn_output\n",
        "            \n",
        "            # MLP block\n",
        "            residual = hidden_states\n",
        "            hidden_states = layer_module.post_attention_layernorm(hidden_states)\n",
        "            hidden_states = layer_module.mlp(hidden_states)\n",
        "            hidden_states = residual + hidden_states\n",
        "            \n",
        "            return hidden_states\n",
        "                \n",
        "        return layer_module(hidden_states)\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self) -> int:\n",
        "        return self.hf_config.hidden_size\n",
        "\n",
        "class LlamaEmbeddingsLayer(nn.Module):\n",
        "    def __init__(self, config, weights):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n",
        "    def forward(self, x): return self.embed_tokens(x)\n",
        "\n",
        "class LlamaHeadLayer(nn.Module):\n",
        "    def __init__(self, config, weights):\n",
        "        super().__init__()\n",
        "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    def forward(self, x): return self.lm_head(self.norm(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile tasks/fine_tune_llama_7b.py\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer\n",
        "from lema import LemaConfig, LemaModel, MemoryStrategy\n",
        "from lema.utils.model_utils import prepare_monolithic_safetensors\n",
        "\n",
        "MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "MODEL_PATH = \"llama2_7b.safetensors\"\n",
        "\n",
        "TRAINING_DATA = [\n",
        "    \"What is photosynthesis? Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "    \"Who was Albert Einstein? Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
        "    \"What is the capital of France? The capital of France is Paris.\",\n",
        "    \"Explain gravity. Gravity is a natural phenomenon by which all things with mass or energy are brought toward one another.\",\n",
        "    \"What is LEMA? LEMA is a framework that virtualizes GPU memory to enable training large models on limited hardware.\",\n",
        "] * 5\n",
        "\n",
        "def fine_tune_llama_7b_task():\n",
        "    print(\"--- STARTING LEMA 7B FINE-TUNING ---\")\n",
        "    \n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"Preparing {MODEL_PATH}...\")\n",
        "        prepare_monolithic_safetensors(MODEL_NAME, MODEL_PATH)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    config = LemaConfig(\n",
        "        model_name_or_path=MODEL_NAME,\n",
        "        gbi_path=MODEL_PATH,\n",
        "        device=\"cuda\",\n",
        "        strategy=MemoryStrategy.STREAMING,\n",
        "        learning_rate=5e-5,\n",
        "        lora_rank=16,\n",
        "        gradient_checkpointing=True\n",
        "    )\n",
        "    \n",
        "    model = LemaModel(config)\n",
        "    model.initialize_lora()\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=config.learning_rate)\n",
        "    trainer = model.get_trainer(optimizer)\n",
        "    \n",
        "    print(f\"\\nTraining on {len(TRAINING_DATA)} examples...\")\n",
        "    start_time = time.time()\n",
        "    for text in TRAINING_DATA:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "        logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "            \n",
        "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    trainer.save_checkpoint(\"output/llama-7b-final\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fine_tune_llama_7b_task()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile tasks/fine_tune_llama_7b_config.py\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from src.lema.core.gbi import GlobalBinaryIndex\n",
        "from src.lema.models.llama import LlamaAdapter\n",
        "from src.lema.engine.trainer import LemaTrainer\n",
        "from src.lema.core.lora import LoRAManager\n",
        "from src.lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "MODEL_NAME = \"NousResearch/Llama-2-7b-hf\"\n",
        "MODEL_PATH = \"llama2_7b.safetensors\"\n",
        "\n",
        "TRAINING_DATA = [\n",
        "    \"What is photosynthesis? Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "    \"Who was Albert Einstein? Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
        "    \"What is the capital of France? The capital of France is Paris.\",\n",
        "    \"Explain gravity. Gravity is a natural phenomenon by which all things with mass or energy are brought toward one another.\",\n",
        "    \"What is LEMA? LEMA is a framework that virtualizes GPU memory to enable training large models on limited hardware.\",\n",
        "] * 10\n",
        "\n",
        "def fine_tune_llama_7b_with_config():\n",
        "    print(\"--- STARTING LEMA 7B FINE-TUNING (Config Object) ---\")\n",
        "    \n",
        "    # 1. Setup Configuration\n",
        "    config = LemaConfig(\n",
        "        model_name_or_path=MODEL_PATH,\n",
        "        device=\"cuda\",\n",
        "        strategy=MemoryStrategy.STREAMING,\n",
        "        lora_rank=16,\n",
        "        lora_alpha=32,\n",
        "        learning_rate=5e-5,\n",
        "        dtype=\"float16\"\n",
        "    )\n",
        "    \n",
        "    # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # HF Config (for model architecture)\n",
        "    hf_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    model_config = hf_config.to_dict()\n",
        "    model_config[\"attn_implementation\"] = config.attn_implementation\n",
        "    model_config[\"torch_dtype\"] = config.dtype\n",
        "    \n",
        "    # Components\n",
        "    adapter = LlamaAdapter(model_config)\n",
        "    gbi = GlobalBinaryIndex(config.gbi_path)\n",
        "    \n",
        "    # LoRA\n",
        "    lora_config_dict = {\n",
        "        \"r\": config.lora_rank, \n",
        "        \"alpha\": config.lora_alpha, \n",
        "        \"target_modules\": config.lora_target_modules\n",
        "    }\n",
        "    lora_manager = LoRAManager(lora_config_dict, device=config.device)\n",
        "    \n",
        "    print(\"Initializing LoRA parameters...\")\n",
        "    for layer in adapter.get_layer_metadata():\n",
        "        if layer['type'] == 'block':\n",
        "            module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "            adapter.release_layer_module(module)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    trainable_params = lora_manager.get_trainable_parameters()\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=config.learning_rate)\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = LemaTrainer(\n",
        "        config=config,\n",
        "        model_adapter=adapter, \n",
        "        gbi=gbi, \n",
        "        lora_manager=lora_manager, \n",
        "        optimizer=optimizer\n",
        "    )\n",
        "    \n",
        "    # Training Loop\n",
        "    print(f\"\\nTraining on {len(TRAINING_DATA)} examples...\")\n",
        "    for epoch in range(1):\n",
        "        total_loss = 0\n",
        "        for i, text in enumerate(TRAINING_DATA):\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            input_ids = inputs[\"input_ids\"].to(config.device)\n",
        "            \n",
        "            logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "            total_loss += loss\n",
        "            \n",
        "            if (i+1) % 10 == 0:\n",
        "                print(f\"Step {i+1}/{len(TRAINING_DATA)} - Current Loss: {loss:.4f}\")\n",
        "                \n",
        "        print(f\"Epoch {epoch+1} - Avg Loss: {total_loss / len(TRAINING_DATA):.4f}\")\n",
        "    \n",
        "    print(\"Fine-tuning with Config Object completed successfully.\")\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        os.remove(MODEL_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile tasks/fine_tune_smollm.py\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer\n",
        "from lema import LemaConfig, LemaModel, MemoryStrategy\n",
        "from lema.utils.model_utils import prepare_monolithic_safetensors\n",
        "\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
        "MODEL_PATH = \"smollm2_1.7b.safetensors\"\n",
        "\n",
        "TRAINING_DATA = [\n",
        "    \"What is photosynthesis? Photosynthesis is the process by which plants use sunlight to synthesize nutrients from carbon dioxide and water.\",\n",
        "    \"Who was Albert Einstein? Albert Einstein was a theoretical physicist who developed the theory of relativity.\",\n",
        "    \"What is the capital of France? The capital of France is Paris.\",\n",
        "] * 10\n",
        "\n",
        "def fine_tune_smollm():\n",
        "    print(\"--- STARTING SMOL-LM FINE-TUNING ---\")\n",
        "    \n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"Preparing {MODEL_PATH}...\")\n",
        "        prepare_monolithic_safetensors(MODEL_NAME, MODEL_PATH)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    config = LemaConfig(\n",
        "        model_name_or_path=MODEL_NAME,\n",
        "        gbi_path=MODEL_PATH,\n",
        "        device=\"cuda\",\n",
        "        strategy=MemoryStrategy.STREAMING,\n",
        "        learning_rate=5e-5,\n",
        "        lora_rank=16\n",
        "    )\n",
        "    \n",
        "    model = LemaModel(config)\n",
        "    model.initialize_lora()\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=config.learning_rate)\n",
        "    trainer = model.get_trainer(optimizer)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for text in TRAINING_DATA:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "        trainer.train_step(input_ids, labels=input_ids)\n",
        "            \n",
        "    print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    trainer.save_checkpoint(\"output/smollm-final\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fine_tune_smollm()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile examples/demo_lema.py\n",
        "import torch\n",
        "import os\n",
        "from lema import LemaConfig, LemaModel, MemoryStrategy\n",
        "from lema.utils.model_utils import break_shared_weights\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "def run_demo():\n",
        "    print(\"--- LEMA Unified API Demo ---\")\n",
        "\n",
        "    model_dir = \"./demo_model\"\n",
        "    gbi_path = os.path.join(model_dir, \"model.safetensors\")\n",
        "    \n",
        "    # 1. Configuration\n",
        "    config = LemaConfig(\n",
        "        model_name_or_path=model_dir, \n",
        "        model_type=\"gpt2\",\n",
        "        gbi_path=gbi_path, \n",
        "        device=\"cpu\",\n",
        "        strategy=MemoryStrategy.STREAMING,\n",
        "        lora_rank=8,\n",
        "        lora_target_modules=[\"c_attn\"],\n",
        "        output_dir=\"./lema_checkpoints\",\n",
        "        save_steps=10\n",
        "    )\n",
        "\n",
        "    # 2. Initialize Model & Trainer\n",
        "    model = LemaModel(config)\n",
        "    model.initialize_lora()\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=1e-4)\n",
        "    trainer = model.get_trainer(optimizer)\n",
        "\n",
        "    # 3. Execution\n",
        "    print(\"Executing training step...\")\n",
        "    input_ids = torch.randint(0, 1000, (1, 16))\n",
        "    logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "    \n",
        "    print(f\"Step complete. Loss: {loss:.4f}\")\n",
        "    trainer.save_checkpoint(\"./lema_checkpoints/final_demo\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_dir = \"./demo_model\"\n",
        "    if not os.path.exists(os.path.join(model_dir, \"model.safetensors\")):\n",
        "        print(\"Generating dummy model...\")\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "        dummy_config = GPT2Config(n_layer=2, n_embd=128, n_head=4, vocab_size=1000)\n",
        "        dummy_model = GPT2LMHeadModel(dummy_config)\n",
        "        dummy_model = break_shared_weights(dummy_model)\n",
        "        \n",
        "        state_dict = {k: v.clone().detach() for k, v in dummy_model.state_dict().items()}\n",
        "        save_file(state_dict, os.path.join(model_dir, \"model.safetensors\"))\n",
        "        dummy_config.save_pretrained(model_dir)\n",
        "    \n",
        "    run_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile examples/benchmark_runner.py\n",
        "import sys\n",
        "import os\n",
        "import resource\n",
        "import torch\n",
        "import time\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "# Add src to path if not installed\n",
        "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"src\")))\n",
        "\n",
        "from lema.core.gbi import GlobalBinaryIndex\n",
        "from lema.models.gpt2 import GPT2Adapter\n",
        "from lema.engine.trainer import LemaTrainer\n",
        "from lema.core.lora import LoRAManager\n",
        "from lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "def get_peak_rss_mb():\n",
        "    # ru_maxrss is in kilobytes on Linux\n",
        "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
        "\n",
        "def run_baseline(model_path):\n",
        "    print(\"--- Running BASELINE ---\")\n",
        "    start_ram = get_peak_rss_mb()\n",
        "    \n",
        "    # 1. Config (Matching dummy_gpt2 if path matches)\n",
        "    if \"dummy\" in model_path:\n",
        "        config = GPT2Config(vocab_size=1000, n_positions=128, n_embd=64, n_layer=4, n_head=4)\n",
        "    else:\n",
        "        config = GPT2Config(vocab_size=50257, n_positions=1024, n_embd=768, n_layer=12, n_head=12)\n",
        "    \n",
        "    # 2. Instantiate Model\n",
        "    print(\"Instantiating Model...\")\n",
        "    model = GPT2LMHeadModel(config)\n",
        "    \n",
        "    # 3. Load Weights\n",
        "    print(\"Loading Weights...\")\n",
        "    from safetensors.torch import load_file\n",
        "    state_dict = load_file(model_path)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    \n",
        "    print(f\"Model loaded. RAM: {get_peak_rss_mb():.2f} MB\")\n",
        "    \n",
        "    # 4. Forward Pass\n",
        "    print(\"Forward Pass...\")\n",
        "    input_ids = torch.randint(0, config.vocab_size, (1, 64))\n",
        "    output = model(input_ids)\n",
        "    \n",
        "    # 5. Backward Pass\n",
        "    print(\"Backward Pass...\")\n",
        "    loss = output.logits.mean()\n",
        "    loss.backward()\n",
        "    \n",
        "    peak_ram = get_peak_rss_mb()\n",
        "    print(f\"Baseline Peak RSS: {peak_ram:.2f} MB\")\n",
        "    return peak_ram\n",
        "\n",
        "def run_lema(model_path):\n",
        "    print(\"--- Running LEMA ---\")\n",
        "    start_ram = get_peak_rss_mb()\n",
        "    \n",
        "    # 1. Config\n",
        "    if \"dummy\" in model_path:\n",
        "        hf_config = {\"vocab_size\": 1000, \"n_positions\": 128, \"n_embd\": 64, \"n_layer\": 4, \"n_head\": 4, \"attn_implementation\": \"eager\"}\n",
        "    else:\n",
        "        hf_config = {\"vocab_size\": 50257, \"n_positions\": 1024, \"n_embd\": 768, \"n_layer\": 12, \"n_head\": 12, \"attn_implementation\": \"eager\"}\n",
        "    \n",
        "    lema_config = LemaConfig(model_name_or_path=model_path, device=\"cpu\", strategy=MemoryStrategy.STREAMING)\n",
        "    \n",
        "    # 2. Components\n",
        "    print(\"Initializing Components...\")\n",
        "    adapter = GPT2Adapter(hf_config)\n",
        "    gbi = GlobalBinaryIndex(model_path)\n",
        "    \n",
        "    # LoRA\n",
        "    lora_config = {\"r\": 8, \"alpha\": 16, \"target_modules\": [\"c_attn\"]}\n",
        "    lora_manager = LoRAManager(lora_config, device=\"cpu\")\n",
        "    \n",
        "    # Init LoRA params\n",
        "    for layer in adapter.get_layer_metadata():\n",
        "        if layer['type'] == 'block':\n",
        "            module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "            adapter.release_layer_module(module)\n",
        "            \n",
        "    optimizer = torch.optim.AdamW(lora_manager.get_trainable_parameters(), lr=1e-4)\n",
        "    trainer = LemaTrainer(lema_config, adapter, gbi, lora_manager=lora_manager, optimizer=optimizer)\n",
        "    \n",
        "    print(f\"Components Ready. RAM: {get_peak_rss_mb():.2f} MB\")\n",
        "    \n",
        "    # 3. Train Step\n",
        "    print(\"Training Step...\")\n",
        "    input_ids = torch.randint(0, hf_config[\"vocab_size\"], (1, 64))\n",
        "    trainer.train_step(input_ids, labels=input_ids)\n",
        "    \n",
        "    peak_ram = get_peak_rss_mb()\n",
        "    print(f\"LEMA Peak RSS: {peak_ram:.2f} MB\")\n",
        "    return peak_ram\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Usage: python examples/benchmark_runner.py [mode] [model_path]\")\n",
        "        sys.exit(1)\n",
        "        \n",
        "    mode = sys.argv[1]\n",
        "    path = sys.argv[2]\n",
        "    \n",
        "    if mode == \"baseline\":\n",
        "        run_baseline(path)\n",
        "    elif mode == \"lema\":\n",
        "        run_lema(path)\n",
        "    else:\n",
        "        print(\"Unknown mode\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile examples/generate_medium_gpt2.py\n",
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from safetensors.torch import save_file\n",
        "import os\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=50257,\n",
        "    n_positions=1024,\n",
        "    n_embd=768,\n",
        "    n_layer=12,\n",
        "    n_head=12\n",
        ")\n",
        "\n",
        "print(\"Creating GPT-2 Small model...\")\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "# Break shared weights\n",
        "model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "\n",
        "print(\"Saving to medium_gpt2.safetensors...\")\n",
        "save_file(model.state_dict(), \"medium_gpt2.safetensors\")\n",
        "\n",
        "size_bytes = os.path.getsize(\"medium_gpt2.safetensors\")\n",
        "print(f\"Created medium_gpt2.safetensors: {size_bytes / 1024 / 1024:.2f} MB\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile examples/generate_dummy_gpt2.py\n",
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=1000,\n",
        "    n_positions=128,\n",
        "    n_embd=64,\n",
        "    n_layer=4,\n",
        "    n_head=4\n",
        ")\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "# Break shared weights for safetensors\n",
        "model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "\n",
        "# Save to safetensors\n",
        "save_file(model.state_dict(), \"dummy_gpt2.safetensors\")\n",
        "print(\"Created dummy_gpt2.safetensors\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile examples/kaggle/benchmark_logic.py\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "# LEMA Imports\n",
        "from src.lema.core.gbi import GlobalBinaryIndex\n",
        "from src.lema.models.llama import LlamaAdapter\n",
        "from src.lema.models.gpt2 import GPT2Adapter\n",
        "from src.lema.engine.trainer import LemaTrainer\n",
        "from src.lema.core.lora import LoRAManager\n",
        "from src.lema.config import LemaConfig, MemoryStrategy\n",
        "\n",
        "print(f\"Using Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# --- MODELS TO TEST ---\n",
        "MODELS = [\n",
        "    {\n",
        "        \"name\": \"GPT2 (Small)\",\n",
        "        \"hf_id\": \"gpt2\",\n",
        "        \"path\": \"gpt2.safetensors\",\n",
        "        \"type\": \"gpt2\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"TinyLlama 1.1B\",\n",
        "        \"hf_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"path\": \"tinyllama_1b.safetensors\",\n",
        "        \"type\": \"llama\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"SmolLM2 1.7B\",\n",
        "        \"hf_id\": \"HuggingFaceTB/SmolLM2-1.7B\",\n",
        "        \"path\": \"smollm2_1.7b.safetensors\",\n",
        "        \"type\": \"llama\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Llama-2 7B\",\n",
        "        \"hf_id\": \"NousResearch/Llama-2-7b-hf\",\n",
        "        \"path\": \"llama2_7b.safetensors\",\n",
        "        \"type\": \"llama\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def download_and_convert(model_info):\n",
        "    print(f\"\\n--- Preparing {model_info['name']} ---\")\n",
        "    if os.path.exists(model_info['path']):\n",
        "        print(f\"{model_info['path']} already exists.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading {model_info['hf_id']}...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_info['hf_id'], \n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"cpu\"\n",
        "    )\n",
        "    \n",
        "    # Break shared weights if necessary\n",
        "    if hasattr(model, \"lm_head\") and hasattr(model, \"model\") and hasattr(model.model, \"embed_tokens\"):\n",
        "         if model.lm_head.weight.data_ptr() == model.model.embed_tokens.weight.data_ptr():\n",
        "             model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "    elif hasattr(model, \"lm_head\") and hasattr(model, \"transformer\") and hasattr(model.transformer, \"wte\"):\n",
        "        # GPT2 shared weights\n",
        "        if model.lm_head.weight.data_ptr() == model.transformer.wte.weight.data_ptr():\n",
        "             model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "\n",
        "    print(f\"Saving to {model_info['path']}...\")\n",
        "    save_file(model.state_dict(), model_info['path'])\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "def run_peft_baseline(model_info):\n",
        "    print(f\"\\n>>> TESTING STANDARD PEFT ON: {model_info['name']} <<<\")\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    \n",
        "    download_and_convert(model_info)\n",
        "    \n",
        "    try:\n",
        "        # Load Model in FP16\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_info['hf_id'],\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"cuda\"\n",
        "        )\n",
        "        \n",
        "        # Configure LoRA\n",
        "        target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"] if model_info['type'] == \"gpt2\" else \\\n",
        "                         [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "                         \n",
        "        peft_config = LoraConfig(\n",
        "            r=16, lora_alpha=32,\n",
        "            target_modules=target_modules,\n",
        "            lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        \n",
        "        model = get_peft_model(model, peft_config)\n",
        "        print(f\"PEFT Trainable params: {model.print_trainable_parameters()}\")\n",
        "        \n",
        "        # Dummy Train Step\n",
        "        input_ids = torch.randint(0, 1000, (1, 128)).cuda()\n",
        "        output = model(input_ids, labels=input_ids)\n",
        "        loss = output.loss\n",
        "        loss.backward()\n",
        "        \n",
        "        peak_vram = torch.cuda.max_memory_allocated() / 1024**3\n",
        "        print(f\"Standard PEFT Peak VRAM: {peak_vram:.2f} GB\")\n",
        "        \n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return peak_vram\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"PEFT Baseline Failed: {e}\")\n",
        "        return float('inf')\n",
        "\n",
        "def run_test(model_info):\n",
        "    print(f\"\\n>>> TESTING LEMA ON: {model_info['name']} <<<\")\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    \n",
        "    download_and_convert(model_info)\n",
        "    \n",
        "    try:\n",
        "        # 1. Config\n",
        "        hf_config = AutoConfig.from_pretrained(model_info['hf_id'])\n",
        "        hf_config_dict = hf_config.to_dict()\n",
        "        hf_config_dict[\"attn_implementation\"] = \"eager\"\n",
        "        hf_config_dict[\"torch_dtype\"] = \"float16\"\n",
        "        \n",
        "        # 2. Components\n",
        "        if model_info['type'] == \"llama\":\n",
        "            adapter = LlamaAdapter(hf_config_dict)\n",
        "        elif model_info['type'] == \"gpt2\":\n",
        "            adapter = GPT2Adapter(hf_config_dict)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown type: {model_info['type']}\")\n",
        "            \n",
        "        gbi = GlobalBinaryIndex(model_info['path'])\n",
        "        \n",
        "        # LEMA Config\n",
        "        lema_config = LemaConfig(\n",
        "            model_name_or_path=model_info['path'],\n",
        "            device=\"cuda\",\n",
        "            strategy=MemoryStrategy.STREAMING,\n",
        "            learning_rate=1e-4\n",
        "        )\n",
        "        \n",
        "        # LoRA - ALL LINEAR LAYERS\n",
        "        target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"] if model_info['type'] == \"gpt2\" else \\\n",
        "                         [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "                         \n",
        "        lora_config = {\n",
        "            \"r\": 16, \"alpha\": 32, \n",
        "            \"target_modules\": target_modules\n",
        "        }\n",
        "        lora_manager = LoRAManager(lora_config, device=\"cuda\")\n",
        "        \n",
        "        # Initialize LoRA params (Fix leak)\n",
        "        print(\"Initializing LoRA parameters...\")\n",
        "        for layer in adapter.get_layer_metadata():\n",
        "            if layer['type'] == 'block':\n",
        "                module = adapter.construct_layer_module(layer['id'], None, lora_manager)\n",
        "                adapter.release_layer_module(module)\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        trainable_params = lora_manager.get_trainable_parameters()\n",
        "        print(f\"Trainable Tensors: {len(trainable_params)}\")\n",
        "        optimizer = torch.optim.AdamW(trainable_params, lr=lema_config.learning_rate)\n",
        "        \n",
        "        trainer = LemaTrainer(\n",
        "            config=lema_config,\n",
        "            model_adapter=adapter, \n",
        "            gbi=gbi, \n",
        "            lora_manager=lora_manager, \n",
        "            optimizer=optimizer\n",
        "        )\n",
        "        \n",
        "        # 3. Execution\n",
        "        print(\"Executing Train Step...\")\n",
        "        # Create dummy inputs based on vocab size\n",
        "        vocab_size = hf_config.vocab_size\n",
        "        input_ids = torch.randint(0, vocab_size, (1, 128)).cuda()\n",
        "        \n",
        "        logits, loss = trainer.train_step(input_ids, labels=input_ids)\n",
        "        \n",
        "        print(f\"Loss: {loss:.4f}\")\n",
        "        peak_vram = torch.cuda.max_memory_allocated() / 1024**3\n",
        "        print(f\"LEMA Peak VRAM: {peak_vram:.2f} GB\")\n",
        "        print(f\"RESULT: {model_info['name']} -> SUCCESS\")\n",
        "        return peak_vram\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"RESULT: {model_info['name']} -> FAILED\")\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return float('inf')\n",
        "    finally:\n",
        "        # Cleanup model file to save disk space on Kaggle\n",
        "        if os.path.exists(model_info['path']):\n",
        "            os.remove(model_info['path'])\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = {}\n",
        "    for model in MODELS:\n",
        "        peft_vram = run_peft_baseline(model)\n",
        "        lema_vram = run_test(model)\n",
        "        results[model['name']] = {\"PEFT\": peft_vram, \"LEMA\": lema_vram}\n",
        "    \n",
        "    print(\"\\n=== FINAL RESULTS (VRAM in GB) ===\")\n",
        "    print(f\"{'Model':<20} | {'PEFT (Baseline)':<15} | {'LEMA (Ours)':<15} | {'Savings':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "    for name, data in results.items():\n",
        "        peft = data[\"PEFT\"]\n",
        "        lema = data[\"LEMA\"]\n",
        "        savings = (1 - lema/peft) * 100 if peft > 0 else 0\n",
        "        print(f\"{name:<20} | {peft:<15.2f} | {lema:<15.2f} | {savings:<10.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile examples/kaggle/speed_benchmark.py\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "# LEMA Unified Imports\n",
        "from lema import LemaConfig, LemaModel, MemoryStrategy\n",
        "from lema.utils.model_utils import prepare_monolithic_safetensors\n",
        "\n",
        "print(f\"Using Transformers version: {transformers.__version__}\")\n",
        "\n",
        "MODELS = [\n",
        "    {\"name\": \"TinyLlama 1.1B\", \"hf_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"path\": \"tinyllama_1b.safetensors\", \"type\": \"llama\"},\n",
        "    {\"name\": \"Llama-2 7B\", \"hf_id\": \"NousResearch/Llama-2-7b-hf\", \"path\": \"llama2_7b.safetensors\", \"type\": \"llama\"}\n",
        "]\n",
        "\n",
        "NUM_STEPS = 20 \n",
        "\n",
        "def benchmark_peft_speed(model_info):\n",
        "    print(f\"\\n>>> BENCHMARKING PEFT SPEED: {model_info['name']} <<<\")\n",
        "    torch.cuda.empty_cache()\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_info['hf_id'], torch_dtype=torch.float16, device_map=\"cuda\")\n",
        "        model.gradient_checkpointing_enable()\n",
        "        peft_config = LoraConfig(\n",
        "            r=16, lora_alpha=32, \n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        model = get_peft_model(model, peft_config)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "        input_ids = torch.randint(0, 1000, (1, 512)).cuda()\n",
        "        \n",
        "        # Warmup\n",
        "        model(input_ids, labels=input_ids).loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        torch.cuda.synchronize()\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for _ in range(NUM_STEPS):\n",
        "            model(input_ids, labels=input_ids).loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        torch.cuda.synchronize()\n",
        "        \n",
        "        avg_time = (time.time() - start_time) / NUM_STEPS\n",
        "        print(f\"PEFT Avg Time/Step: {avg_time:.4f}s\")\n",
        "        return avg_time\n",
        "    except Exception as e:\n",
        "        print(f\"PEFT Benchmark Failed: {e}\")\n",
        "        return float('inf')\n",
        "\n",
        "def benchmark_lema_speed(model_info):\n",
        "    print(f\"\\n>>> BENCHMARKING LEMA SPEED: {model_info['name']} <<<\")\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if not os.path.exists(model_info['path']):\n",
        "        print(f\"Preparing {model_info['path']}...\")\n",
        "        prepare_monolithic_safetensors(model_info['hf_id'], model_info['path'])\n",
        "    \n",
        "    use_gc = \"7b\" in model_info['hf_id'].lower()\n",
        "    \n",
        "    try:\n",
        "        config = LemaConfig(\n",
        "            model_name_or_path=model_info['hf_id'],\n",
        "            gbi_path=model_info['path'],\n",
        "            device=\"cuda\",\n",
        "            strategy=MemoryStrategy.STREAMING,\n",
        "            learning_rate=1e-4,\n",
        "            gradient_checkpointing=use_gc,\n",
        "            lora_rank=16\n",
        "        )\n",
        "        \n",
        "        model = LemaModel(config)\n",
        "        model.initialize_lora()\n",
        "        \n",
        "        optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=config.learning_rate)\n",
        "        trainer = model.get_trainer(optimizer)\n",
        "        \n",
        "        input_ids = torch.randint(0, 1000, (1, 512)).cuda()\n",
        "        trainer.train_step(input_ids, labels=input_ids) # Warmup\n",
        "        torch.cuda.synchronize()\n",
        "        \n",
        "        start_time = time.time()\n",
        "        for _ in range(NUM_STEPS):\n",
        "            trainer.train_step(input_ids, labels=input_ids)\n",
        "        torch.cuda.synchronize()\n",
        "        \n",
        "        avg_time = (time.time() - start_time) / NUM_STEPS\n",
        "        print(f\"LEMA Avg Time/Step: {avg_time:.4f}s\")\n",
        "        return avg_time\n",
        "    except Exception as e:\n",
        "        print(f\"LEMA Benchmark Failed: {e}\")\n",
        "        return float('inf')\n",
        "    finally:\n",
        "        if os.path.exists(model_info['path']): os.remove(model_info['path'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = {}\n",
        "    for model in MODELS:\n",
        "        peft = benchmark_peft_speed(model)\n",
        "        lema = benchmark_lema_speed(model)\n",
        "        results[model['name']] = {\"PEFT\": peft, \"LEMA\": lema}\n",
        "    \n",
        "    print(\"\\n=== UNIFIED SPEED BENCHMARK RESULTS ===\")\n",
        "    for name, data in results.items():\n",
        "        p, l = data[\"PEFT\"], data[\"LEMA\"]\n",
        "        overhead = (l / p) if p > 0 and p != float('inf') else float('inf')\n",
        "        print(f\"{name}: PEFT={p:.4f}s, LEMA={l:.4f}s, Overhead={overhead:.2f}x\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### docs/ARCHITECTURE.md\n",
        "\n",
        "# LEMA Architecture\n",
        "\n",
        "This document describes the internal mechanics of the Layer-wise Efficient Memory Abstraction (LEMA) framework.\n",
        "\n",
        "## The Problem: The VRAM Wall\n",
        "Standard fine-tuning (even with PEFT/LoRA) requires the entire model weights to be resident in VRAM. For a Llama-2 7B model in FP16, this is ~14GB. Adding optimizer states and activations quickly exceeds the capacity of consumer GPUs (e.g., 16GB).\n",
        "\n",
        "## The LEMA Solution: Virtualization\n",
        "LEMA treats GPU VRAM not as a static storage for the model, but as a **dynamic cache** for execution.\n",
        "\n",
        "### 1. The Triple-Buffer Strategy\n",
        "LEMA hides data transfer latency by pipelining movements across three memory tiers:\n",
        "\n",
        "1.  **Storage (NVMe)**: Weights reside in `.safetensors` files. Accessed via `mmap` (Zero-copy).\n",
        "2.  **System RAM (Pinned)**: Acting as a \"Prefetch Buffer\". Pinned memory ensures high-speed Host-to-Device (H2D) transfers.\n",
        "3.  **VRAM (Execution)**: Divided into two \"Slots\" (Active and Prefetch).\n",
        "\n",
        "### 2. The Execution Pipeline\n",
        "While the GPU is computing Layer $N$ in Slot A, LEMA is:\n",
        "-   Asynchronously transferring Layer $N+1$ from RAM to Slot B (VRAM).\n",
        "-   Loading Layer $N+2$ from Disk to RAM (Staging).\n",
        "\n",
        "When Layer $N$ finishes, the slots swap instantly.\n",
        "\n",
        "### 3. The LEMA-Loop (Training Logic)\n",
        "\n",
        "#### Forward Pass\n",
        "-   Model is executed layer-by-layer.\n",
        "-   Only \"Boundary Activations\" (the output of each layer) are stored in VRAM.\n",
        "-   Intermediate activations are discarded.\n",
        "\n",
        "#### Backward Pass\n",
        "-   LEMA traverses the layers in reverse.\n",
        "-   For each layer:\n",
        "    1.  The weights are swapped back into VRAM.\n",
        "    2.  The layer's forward pass is **re-executed** (Segmented Gradient Checkpointing) using the stored boundary activations.\n",
        "    3.  Gradients are calculated for the LoRA adapters.\n",
        "    4.  Optimizer states for those specific adapters are updated.\n",
        "\n",
        "### 4. GBI (Global Binary Index)\n",
        "LEMA uses a specialized indexer to bypass standard PyTorch/Pickle deserialization. By reading the `.safetensors` header, LEMA knows the exact byte offsets for every parameter, allowing it to \"slice\" the file and load only the parameters needed for the current layer module.\n",
        "\n",
        "## Performance Trade-offs\n",
        "-   **VRAM Efficiency**: ~50-70% reduction for 7B+ models.\n",
        "-   **Compute Overhead**: 1.2x - 1.8x slowdown compared to fully resident training, depending on PCIe bandwidth and disk speed.\n",
        "-   **System RAM**: Requires space equal to the model size (or less if using aggressive disk streaming).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### docs/LEMA Framework Proposal.md\n",
        "\n",
        "# **LEMA: Layer-wise Efficient Memory Abstraction**\n",
        "\n",
        "**Architectural Specification for VRAM-Efficient Model Fine-Tuning**\n",
        "\n",
        "## **1\\. Executive Summary**\n",
        "\n",
        "LEMA is a specialized framework designed to facilitate the fine-tuning of Large Language Models (LLMs) on hardware where model size exceeds available VRAM. Unlike standard frameworks that require the full model to be resident in GPU memory, LEMA treats the model as a collection of discrete, addressable binary segments. By implementing a virtualized memory abstraction layer, LEMA performs asynchronous pre-fetching of layers into VRAM, effectively trading PCIe bandwidth for memory headroom.\n",
        "\n",
        "## **2\\. Core Concepts**\n",
        "\n",
        "### **2.1 Global Binary Index (GBI)**\n",
        "\n",
        "Standard model loading (e.g., PyTorch .bin or .pt) involves full deserialization into System RAM. LEMA uses a **Global Binary Index (GBI)**.\n",
        "\n",
        "* **Zero-Copy Mapping:** Uses mmap to map the model file (preferably in .safetensors format) into the process's virtual address space.  \n",
        "* **Header Indexing:** A JSON/Binary header stores the (offset, size, dtype, shape) for every tensor, allowing O(1) access to specific layer weights without scanning the file.\n",
        "\n",
        "### **2.2 Layer-wise Execution (Patchwork)**\n",
        "\n",
        "Instead of a monolithic model.forward(), LEMA decomposes the computational graph into a sequence of isolated layer blocks.\n",
        "\n",
        "* **Weight Swapping:** Only the current layer ![][image1] and the next layer ![][image2] occupy VRAM.  \n",
        "* **Persistence:** Model weights remain frozen in System RAM/Disk; only LoRA adapters are maintained in active memory.\n",
        "\n",
        "## **3\\. The Memory Pipeline (The Triple-Buffer Strategy)**\n",
        "\n",
        "LEMA orchestrates data movement across three tiers to hide the latency of PCIe transfers.\n",
        "\n",
        "| Tier | Residency | Role |\n",
        "| :---- | :---- | :---- |\n",
        "| **Storage (NVMe)** | Global Binary File | The source of truth. Accessed via mmap. |\n",
        "| **System RAM** | Pinned Memory Buffers | The staging area for the next 2-3 layers. |\n",
        "| **VRAM** | Active Slot / Prefetch Slot | The execution zone. |\n",
        "\n",
        "### **Asynchronous Prefetching Logic**\n",
        "\n",
        "1. **Compute Stream:** GPU calculates the forward pass for Layer ![][image3].  \n",
        "2. **Transfer Stream:** Simultaneously, the CPU pushes Layer ![][image4] from Pinned RAM to a reserved VRAM buffer.  \n",
        "3. **Synchronization:** When Layer ![][image3] finishes, the pointers are swapped. Layer ![][image3] is discarded (or moved to RAM if activations are needed), and ![][image4] begins immediate execution.\n",
        "\n",
        "## **4\\. Training Mechanics: The \"LEMA-Loop\"**\n",
        "\n",
        "### **4.1 Forward Pass (Activation Management)**\n",
        "\n",
        "To save VRAM, LEMA implements **Segmented Gradient Checkpointing**:\n",
        "\n",
        "* Instead of storing activations for all 32 layers, LEMA stores only the \"Boundary Activations\" (the output of each chunk).  \n",
        "* Inner-layer activations are discarded and re-computed during the backward pass.\n",
        "\n",
        "### **4.2 Backward Pass (The Reverse Swap)**\n",
        "\n",
        "1. Load Layer ![][image3] weights \\+ LoRA adapters.  \n",
        "2. Retrieve Boundary Activation for Layer ![][image5].  \n",
        "3. Re-run forward pass for Layer ![][image3] to get local activations.  \n",
        "4. Calculate gradients for Layer ![][image3] LoRA adapters.  \n",
        "5. Offload Layer ![][image3] weights; move to Layer ![][image5].\n",
        "\n",
        "### **4.3 Optimizer Offloading**\n",
        "\n",
        "The **Adam Optimizer states** (Momentum and Variance) are stored in System RAM. During the weight update step, LEMA pulls only the specific optimizer slice for the current layer's adapters into VRAM, performs the update, and pushes it back.\n",
        "\n",
        "## **5\\. Technical Implementation Stack**\n",
        "\n",
        "* **Host Language:** Python (Orchestration) / C++ (High-speed Memory Management).  \n",
        "* **Backend:** CUDA / LibTorch.  \n",
        "* **File Format:** safetensors (Native support for zero-copy mmap).  \n",
        "* **Memory Management:** \\* torch.cuda.Stream for non-blocking transfers.  \n",
        "  * tensor.pin\\_memory() to ensure fast Host-to-Device (H2D) throughput.\n",
        "\n",
        "## **6\\. Comparison with Existing Solutions**\n",
        "\n",
        "| Metric | Standard LoRA | LEMA |\n",
        "| :---- | :---- | :---- |\n",
        "| **VRAM Requirement** | Full Model \\+ Gradients | \\~2 Layers \\+ Buffers |\n",
        "| **System RAM Usage** | Model Size | Model Size (via mmap/Page Cache) |\n",
        "| **Speed** | 100% (Baseline) | 30-70% (PCIe Latency) |\n",
        "| **Model Scalability** | Limited by GPU VRAM | Limited by Disk Space |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### docs/USER_GUIDE.md\n",
        "\n",
        "# LEMA User Guide\n",
        "\n",
        "This guide covers common workflows for fine-tuning Large Language Models using LEMA on memory-constrained hardware.\n",
        "\n",
        "## 1. Preparing Your Model\n",
        "\n",
        "LEMA requires model weights in `.safetensors` format. If your model is in PyTorch `.bin` format, you should convert it first.\n",
        "\n",
        "### Shared Weights Warning\n",
        "When saving to `.safetensors`, ensure that shared weights (like `lm_head.weight` and `embed_tokens.weight`) are cloned into distinct tensors, as `safetensors` does not support memory sharing.\n",
        "\n",
        "```python\n",
        "# Quick conversion snippet\n",
        "from transformers import AutoModelForCausalLM\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"your-model-id\")\n",
        "# Break shared weights\n",
        "if model.lm_head.weight.data_ptr() == model.model.embed_tokens.weight.data_ptr():\n",
        "    model.lm_head.weight = torch.nn.Parameter(model.lm_head.weight.clone())\n",
        "\n",
        "save_file(model.state_dict(), \"model.safetensors\")\n",
        "```\n",
        "\n",
        "## 2. Fine-Tuning Workflow\n",
        "\n",
        "The standard workflow involves four steps: Configuration, Initialization, Training, and Saving.\n",
        "\n",
        "### Basic Example\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from lema import LemaConfig, LemaModel, LemaTrainer\n",
        "\n",
        "# 1. Setup Config\n",
        "config = LemaConfig(\n",
        "    model_name_or_path=\"NousResearch/Llama-2-7b-hf\",\n",
        "    gbi_path=\"llama2_7b.safetensors\",\n",
        "    lora_rank=16,\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "# 2. Initialize\n",
        "model = LemaModel(config)\n",
        "model.initialize_lora() # Crucial for new models\n",
        "\n",
        "# 3. Training\n",
        "optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=1e-4)\n",
        "trainer = model.get_trainer(optimizer)\n",
        "\n",
        "for batch in dataloader:\n",
        "    logits, loss = trainer.train_step(batch['input_ids'], labels=batch['labels'])\n",
        "    print(f\"Loss: {loss}\")\n",
        "\n",
        "# 4. Save\n",
        "trainer.save_checkpoint(\"checkpoints/lema-llama-7b-v1\")\n",
        "```\n",
        "\n",
        "## 3. Architecture Specifics\n",
        "\n",
        "When using LEMA, ensure your `lora_target_modules` in `LemaConfig` match your model's architecture:\n",
        "- **Llama**: `[\"q_proj\", \"v_proj\", ...]` (Default)\n",
        "- **GPT-2**: `[\"c_attn\"]`\n",
        "\n",
        "## 4. Memory Strategies\n",
        "\n",
        "LEMA supports two primary strategies in `LemaConfig`:\n",
        "\n",
        "- **`MemoryStrategy.STREAMING` (Default)**: \n",
        "    - **Path**: Disk -> Pinned RAM -> VRAM.\n",
        "    - **Pros**: Lowest VRAM usage. Can fit models much larger than System RAM if needed (via `mmap`).\n",
        "    - **Cons**: Higher latency due to PCIe/Disk bottleneck.\n",
        "- **`MemoryStrategy.RESIDENT`**:\n",
        "    - **Path**: RAM -> VRAM.\n",
        "    - **Pros**: Faster than streaming. Model weights stay in RAM.\n",
        "    - **Cons**: Requires enough System RAM to hold the full model weights (~14GB for a 7B FP16 model).\n",
        "\n",
        "## 4. Tips for Maximum Efficiency\n",
        "\n",
        "1. **Gradient Checkpointing**: Always enable `gradient_checkpointing=True` for 7B+ models. This significantly reduces VRAM usage during the backward pass by not storing intermediate activations.\n",
        "2. **Pinned Memory**: LEMA automatically uses pinned memory for transfers. Ensure your system has sufficient RAM available for the staging buffers (~2x the size of the largest layer).\n",
        "3. **NVMe Storage**: When using `STREAMING` mode, placing your `.safetensors` file on an NVMe SSD will greatly reduce the \"Streaming Overhead\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### docs/API_REFERENCE.md\n",
        "\n",
        "# LEMA API Reference\n",
        "\n",
        "This document provides detailed information about the LEMA (Layer-wise Efficient Memory Abstraction) library API.\n",
        "\n",
        "## Core API\n",
        "\n",
        "### `LemaModel`\n",
        "The primary entry point for the framework. It orchestrates memory management, adapters, and LoRA parameters.\n",
        "\n",
        "#### `__init__(config: LemaConfig)`\n",
        "Initializes the model using a `LemaConfig` object.\n",
        "\n",
        "#### `get_trainer(optimizer: torch.optim.Optimizer)`\n",
        "Returns a `LemaTrainer` instance pre-configured with this model's components and memory manager.\n",
        "\n",
        "#### `initialize_lora()`\n",
        "Pre-initializes all LoRA adapters. Must be called before `get_trainable_parameters()` for new models.\n",
        "\n",
        "#### `get_trainable_parameters()`\n",
        "Returns a list of all trainable parameters (LoRA weights) managed by the model.\n",
        "\n",
        "#### `save_pretrained(save_directory: str)`\n",
        "Saves the configuration and LoRA adapter weights.\n",
        "\n",
        "#### `from_pretrained(path: str, **kwargs)` (Class Method)\n",
        "Loads a LEMA model from a directory containing `lema_config.json` and `adapter_model.bin`.\n",
        "\n",
        "---\n",
        "\n",
        "### `LemaConfig`\n",
        "Configuration dataclass for LEMA.\n",
        "\n",
        "| Parameter | Type | Default | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `model_name_or_path` | `str` | Required | HuggingFace ID or path to model directory. |\n",
        "| `model_type` | `str` | `None` | `llama` or `gpt2`. Auto-detected if None. |\n",
        "| `gbi_path` | `str` | `None` | Path to the `.safetensors` file. |\n",
        "| `device` | `str` | `\"cuda\"` | Execution device. |\n",
        "| `strategy` | `MemoryStrategy` | `STREAMING` | `STREAMING` or `RESIDENT`. |\n",
        "| `save_steps` | `int` | `500` | Steps between automatic checkpoints. |\n",
        "| `output_dir` | `str` | `\"output\"` | Directory for automatic checkpoints. |\n",
        "| `lora_rank` | `int` | `16` | LoRA rank (r). |\n",
        "| `lora_alpha` | `int` | `32` | LoRA alpha. |\n",
        "| `learning_rate` | `float` | `1e-4` | Learning rate. |\n",
        "| `gradient_checkpointing`| `bool` | `False` | Enable to save activation VRAM. |\n",
        "\n",
        "---\n",
        "\n",
        "### `LemaTrainer`\n",
        "Orchestrates the training loop with layer-swapping logic.\n",
        "\n",
        "#### `__init__(config, model_adapter, gbi, lora_manager=None, optimizer=None, memory_manager=None)`\n",
        "Low-level constructor. Preferred usage is via `LemaModel.get_trainer()`.\n",
        "\n",
        "#### `train_step(inputs: torch.Tensor, labels: torch.Tensor = None)`\n",
        "Executes one forward and backward pass. Tracks `global_step` and triggers auto-checkpointing.\n",
        "- Returns: `(logits, loss_value)`.\n",
        "\n",
        "#### `save_checkpoint(save_directory: str)`\n",
        "Saves the model state, configuration, and optimizer state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### docs/BENCHMARK_RESULTS.md\n",
        "\n",
        "# LEMA Benchmark Results (v0.7 - Release Candidate)\n",
        "\n",
        "Benchmarks were performed on **Kaggle (Tesla P100 GPU, 16GB VRAM)**.\n",
        "Comparisons were made between **Standard PEFT (LoRA)** and **LEMA (Streaming Strategy)**.\n",
        "\n",
        "## 1. VRAM Usage (Memory Efficiency)\n",
        "\n",
        "LEMA demonstrates significant VRAM savings, particularly for larger models where the overhead of optimization states and activations usually causes OOM errors.\n",
        "\n",
        "![VRAM Benchmark](assets/vram_benchmark.png)\n",
        "\n",
        "### Detailed Metrics\n",
        "\n",
        "| Model | Parameters | Standard PEFT VRAM | LEMA VRAM | Savings |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **GPT-2 (Small)** | 124M | 0.44 GB | 1.05 GB | N/A* |\n",
        "| **TinyLlama** | 1.1B | 2.67 GB | **2.12 GB** | **20.5%** |\n",
        "| **SmolLM2** | 1.7B | 3.88 GB | **3.20 GB** | **17.6%** |\n",
        "| **Llama-2** | 7B | **13.99 GB** (Load Only)** | **5.90 GB** | **57.9%** |\n",
        "\n",
        "*\\*Note on GPT-2: For extremely small models, LEMA's fixed buffering overhead exceeds the model size. LEMA is optimized for Large-scale models.*\n",
        "*\\**Note on Llama-2 7B: Standard PEFT can load the model (13.99GB) but fails immediately with **Out-Of-Memory (OOM)** when attempting a training step due to gradients/activations. LEMA trains comfortably with >10GB headroom.*\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Training Speed (Throughput)\n",
        "\n",
        "LEMA trades execution speed for memory capability. The architecture involves moving weights from system RAM to VRAM for every layer, introducing latency.\n",
        "\n",
        "![Speed Benchmark](assets/speed_benchmark.png)\n",
        "\n",
        "### Detailed Metrics\n",
        "\n",
        "| Model | PEFT Speed (s/step) | LEMA Speed (s/step) | Overhead Factor | Status |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **TinyLlama 1.1B** | 0.46 s | 1.45 s | **3.1x** | Usable |\n",
        "| **Llama-2 7B** | **FAILED (OOM)** | **7.21 s** | **N/A** | **Enabling** |\n",
        "\n",
        "**Analysis**:\n",
        "- For models that fit in VRAM (1.1B), LEMA introduces a ~3x overhead due to Python-based stream orchestration and PCIe transfer latency.\n",
        "- For models that **do not fit** (7B on 16GB cards), LEMA provides infinite speedup by enabling training where it was previously impossible.\n",
        "\n",
        "## 3. Configuration Used\n",
        "\n",
        "- **LoRA Targets**: All linear layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj` for Llama).\n",
        "- **Sequence Length**: 512.\n",
        "- **Precision**: FP16.\n",
        "- **Gradient Checkpointing**: Enabled for 7B, Disabled for smaller models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath('src'))\n",
        "!pip install -q safetensors accelerate peft transformers\n",
        "print('LEMA Environment Fully Loaded.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN SPEED BENCHMARK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%run examples/kaggle/speed_benchmark.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN SMOL-LM FINE-TUNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%run tasks/fine_tune_smollm.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN LLAMA-7B FINE-TUNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%run tasks/fine_tune_llama_7b.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}